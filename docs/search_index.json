[["index.html", "Technical Manual for Global Soil Nutrient and Nutrient Budgets map (GSNmap) Foreword", " Technical Manual for Global Soil Nutrient and Nutrient Budgets map (GSNmap) FAO Foreword "],["abbreviations-and-acronyms.html", "Abbreviations and acronyms", " Abbreviations and acronyms BD Bulk Density CO2 Carbon dioxide CRF Coarse fragments DM Dry matter DSM Digital soil mapping GAUL Global Administrative Unit Layers GHG Greenhouse gas GSOCmap Global Soil Organic Carbon Map GSOCseq Global Soil Organic Carbon Sequestration Potential Map GSP Global Soil Partnership HWSD Harmonized World Soil Database ISCN International Soil Carbon Network INSII International Network of Soil Information Institutions IPBES Intergovernmental Platform on Biodiversity and Ecosystem Services IPCC Intergovernmental Panel on Climate Change IPR Intellectual Property Rights ITPS Intergovernmental Technical Panel on Soils LDN Land Degradation Neutrality NDVI Normalized difference in vegetation index NPP Net Primary Production P4WG Pillar 4 Working Group QA/QC Quality Assurance/Quality Check RMSE Root mean square error SDF Soil Data Facility SDG Sustainable Development Goals SISLAC Latin America and the Caribbean’s Soil Information System SOC Soil organic carbon SOM Soil organic matter SPADE/M Soil Profile Analytical Database of Europe of Measured Parameters SWRS Status of World’s Soil Resources UNCCD United Nations Convention to Combat Desertification WFS Web Feature Service WoSIS World Soil Information Service "],["contributors.html", "Contributors", " Contributors Prepared by: Global Soil Partnership Secretariat Ronald Vargas Second Intergovernmental Technical Panel on Soils Luca Montanarella - European Commission, Joint Research Centre (Chair); Saéb AbdelHaleem Khresat Third Intergovernmental Technical Panel on Soils Rosa Poch - Spain (Chair); Nsalambi V. Nkongolo - Democratic Republic of the Congo; "],["presentation.html", "Chapter 1 Presentation 1.1 Background and objectives 1.2 Global Soil Partnership 1.3 Country-driven approach and tasks 1.4 How to use this book", " Chapter 1 Presentation 1.1 Background and objectives Soil nutrient availability can affect ecosystem carbon cycling, plant phenology, plant diversity and community composition, plant-herbivore and plant-soil-microbe interactions, as well as the structure of trophic food webs (van Sundert et al., 2020). Thus, the broad range of effects of nutrient availability also affects ecosystem functioning in face of global change, for instance the response of plants to elevated levels of CO2 (Vicca et al., 2018). In the context of agriculture, nutrient availability modulates crop productivity and thus food production. However, the COVID-19 pandemic, current conflicts and devastating extreme weather events triggered by climate change jeopardise achieving sustainable development goal (SDG) 2 (Zero Hunger) by 2030. To date, a total number of around 2.3 billion people are affected by moderate and severe food insecurity (FAO et al., 2022). Despite soil nutrient status and availability being vital to the provisioning of ecosystem services, globally-accessible and harmonised datasets on soil nutrient stocks and soil properties that govern nutrient availability are missing. Therefore, the current global situation requires an increase of food production while preserving natural (soil) resources, lowering greenhouse gas emissions and optimising the use of goods such as fertilisers on agricultural sites (Eisenstein, 2020). Fertiliser prices more than doubled within one year and grain prices increased by around 25 percent (Jan. 2021 - Jan. 2022) (Hebebrand and Laborde, 2022). With the start of the armed conflict in Ukraine in February 2022, this trend became more pronounced. Growing food insecurity and rapidly increasing fertiliser prices underscore the urgent need for informed decision-making and optimised soil nutrient management. However, a large data gap exists in regards to soil nutrient stocks and soil properties that govern nutrient availability. Therefore, FAO’s Global Soil Partnership (GSP) has launched the Global Soil Nutrient and Nutrient Budget map (GSNmap) initiative in an endeavour to provide harmonised and finely resolved soil nutrient data and information to stakeholders following a country-driven approach. Up-to-date soil data on the status and spatial trends of soil nutrients and related soil attributes is key to guide policy-making to close yield gaps, and protect local natural resources. Therefore, locally-specific optimisation of soil nutrient and agricultural management are needed (Cunningham et al., 2013). The soil information collected in the GSNmap thereby serves as a cornerstone in delineating priority areas for action and thereby seizes the opportunity to reduce food insecurity, close yield gaps, and reduce environmental costs arising from mismanagement of soil nutrients and especially overfertilisation. 1.2 Global Soil Partnership The Global Soil Partnership (GSP) was established in December 2012 as a mechanism to develop a strong interactive partnership and to enhance collaboration and generate synergies between all stakeholders to raise awareness and protect the world’s soil resources. From land users to policymakers, one of the main objectives of GSP is to improve governance and promote sustainable management of soils. Since its creation, GSP has become an important partnership platform where global soil issues are discussed and addressed by multiple stakeholders at different levels. The mandate of GSP is to improve governance of the planet’s limited soil resources in order to guarantee productive agricultural soils for a food-secure world. In addition, it supports other essential soil ecosystem services in accordance with the sovereign right of each Member State over its natural resources. In order to achieve its mandate, GSP addresses six thematic action areas to be implemented in collaboration with its regional soil partnerships (Figure 1). The area of work on Soil Information and Data (SID) of the GSP builds an enduring and authoritative global system (GloSIS) to monitor and forecast the condition of the Earth’s soil resources and produce map products at the global level. The secretariat is working with the international network of soil data providers (INSII - International Network of Soil Information Institutions) to implement data related activities. 1.3 Country-driven approach and tasks The GSNmap initiative will be jointly implemented by the International Network of Soil Information Institutions (INSII) and the GSP Secretariat. The process will be country-driven, involving and supporting all Member States in developing their national GSNmap data products. The GSNmap products will be developed following a two phase approach: Phase I: development of soil nutrient and associated soil property maps; Phase II: quantification, analysis, projections of nutrient budgets for agricultural land use systems at national, regional and global scale. These guidelines only concern GSNmap Phase I, while the guidelines for the GSNmap Phase II will be published in the fourth quarter of 2022. Depending on national data availability and technical capacities, ad-hoc solutions will be developed by the GSNmap WG to support countries during the national GSNmap production and/or harmonisation phase. Where possible, GSP Secretariat will use publicly available data to gap-fill the areas which are not covered by the national submissions unless the country requests to be left blank on the GSNmap products. 1.4 How to use this book The present document is a technical manual on the phase I of the GSNmap initiative. It provides the scientific background on the importance of soil nutrients and guidance on the digital soil mapping techniques to map nutrients and soil properties that govern nutrient availability. It also comprises a compendium with all necessary scripts to generate national GSNmaps. Chapter 1 provides general information about the GSNmap initiative as another activity of the GSP. Chapter 2 focuses on the scientific state-of-the-art in terms of soil nutrients and soil nutrient mapping. Chapter 3 and 4 introduce the software requirements and the concept of digital soil mapping. Chapter 5 to 8 guide the reader through the nutrient mapping exercise of GSNmap Phase I providing step-by-step instructions. Chapter 9 explains how the national GSNmaps are reported to the GSP. Chapter 10 serves as a repository for the complete scripts needed for the GSNmap. The GSNmap Technical Manual is structured as a practical document to be used by national experts in the endeavour to employ digital soil mapping techniques to generate national nutrient maps based on a common methodology. The concept of digital soil mapping presented here can however be also used in mapping exercises that focus on other soil properties and is therefore also relevant to scientists and digital soil mappers. "],["soil-nutrients.html", "Chapter 2 Soil Nutrients 2.1 Definition of soil nutrients 2.2 Soil properties governing nutrient availability", " Chapter 2 Soil Nutrients 2.1 Definition of soil nutrients In theory, soil nutrients are defined as those chemical elements that are essential to plant growth (von Liebig, 1841; Arnon and Stout, 1939). First, von Liebig (1841) declared nitrogen (N), sulphur (S), phosphorus (P), potassium (K), calcium (Ca), magnesium (Mg), silicon (Si), sodium (Na) and iron (Fe) as being essential. However, these findings lacked experimental research and were based on merely observational studies. Furthermore, if plant uptake is the only criteria for essentiality, the definition disregards the fact that plants also take up innecessary or even toxic elements. Therefore, stricter criteria such as the one by Arnon and Stout (1939) were defined. They postulated that three criteria need to be met for an essential mineral nutrients (Kirkby, 2012): 1. Nutrient must be required by plants to complete their life cycle; 2. Nutrient must be irreplaceable; and 3. Nutrient must be involved in the plant metabolism. Following this defintion to date the following nutrients would be considered essential for higher plants (Mengel et al., 2001): carbon (C), hydrogen (H), oxygen (O), N, P, S, cobalt (Co), K, Ca, Mg, Fe, manganese (Mn), copper (Cu), Si, zinc (Zn), molybdenum (Mo), boron (B), chlorine (Cl), nickel (Ni), Na. However, Co, Si, Ni, and Na are not considered essential for all plants. Other definitions used biochemical functions for classification purposes (Mengel and Kirkby, 2001). Here, four nutrient groups are distinguished: 1. major constituents of organic material (C, H, O, N, S); 2. nutrients that are involved in esterification of alcohol groups (P, B, Si); 3. nutrients that establish an osmotic potential (ions) (K, Na, Ca, Mg, Mn, Cl); and 4. nutrients that enable electron transport (ions or chelates) (Fe, Cu, Zn, Mo). Still, the most common classification of soil nutrients is based on the absolute quantities of an element that a plant takes up resulting in macro- and micronutrients (Mengel &amp; Kirkby, 2012). Despite being widely used, the definition has several shortcomings as also toxic elements can be taken up in greater quantities (e.g. Al). Furthermore, the threshold definition between macro- and micronutrients is somewhat arbitrary (Mengel &amp; Kirkby, 2012). It is important to point out that the discussion on how to accurately define essential nutrients is ongoing as recent contribution to the topic show (Brown et al., 2022). The generation of the GSNmaps is oriented by the recently published report on the state of the art of soils for nutrition (FAO, 2022) and is shown in Table XX. It is based on the contribution of each element to the average plant content. Table XX: Classification of macro- and micronutrients by FAO (2022). Macronutrients Micronutrients N Fe P Mn K Zn Ca Cu Mg B S Mo - Cl 2.2 Soil properties governing nutrient availability The uptake of nutrients by plants is regulated in parts by the organism itself as for instance shoot growth is coupled with root growth (Wang et al., 2007). Still, soil properties mediate nutrient mobility and conditions at the plant-soil interface. The most important soil properties that determine nutrient availability are physicochemical properties such as soil pH, cation exchange capacity (CEC), soil texture, soil organic matter (SOM) content, and bulk density (BD). Most nutrients are taken up in their ionized form (Robertson et al., 1999). Therefore, the chemical characterization of the soil solution is key to understand nutrient dynamics and uptake. Soil pH, as a measure of exchangeable hydrogen protons (H+), is a crucial parameter to determine the acidity of the soil solution that can inhibit or mediate nutrient uptake. For instance, very low pH values of around 4 decreased the uptake of (basic) cations such as Ca or Mg by paddy rice, wheat, corn, common bean and cowpea whereas lower pH values favoured the uptake of Zn, Fe, and Mn. At higher pH values the uptake of cations was enhanced (Fageria &amp; Zimmermann, 2008). The CEC, as a measure of exchangeable cations (e.g K+, Mg2+, Ca2+, etc.) available in the soil solution and attached to soil particles is a complementary parameter of nutrient availability in soils (Robertson et al., 1999). The CEC informs on the capacity of soils to retain positively charged nutrients (basic cations) and thus gives information on how strong a soil can buffer subsequent acidification. This retention and buffer capacity is strongly linked to soil texture. High clay contents usually lead to higher CEC and thus higher cation retention. Conversely, sandy textured soils strongly rely on soil organic matter (SOM) content that has high CEC to retain cations. SOM content further augments aeration of soils due to its low density and provides high specific surface area to retain nutrients. Finally, BD is key to nutrient availability as it governs facilitates or inhibits root growth and thus nutrient uptake by plants. Due to its impact on soil porosity, BD also governs microbial activity (through aeration) and water infiltration that defines nutrient mobility. "],["setting-up-the-software-environment.html", "Chapter 3 Setting-up the software environment 3.1 Use of R, RStudio and R Packages 3.2 R packages 3.3 GEE - google earth engine 3.4 rgee - Extension to use google earth engine in R", " Chapter 3 Setting-up the software environment This chapter provides an overview on the software required to map soil nutrients and associated soil properties. The tools are open source and can be downloaded and installed by users following the steps that are described here. The instructions given are for Microsoft Windows operational systems. Instructions for other operational systems (e.g. Linux Flavours, MacOS) can be found through free online resources. 3.1 Use of R, RStudio and R Packages R is a language and environment for statistical computing created in 1992. It provides a wide variety of statistical (e.g. linear modeling, statistical tests, time-series, classification, clustering, etc.) and graphical methods, and has been constantly extended by an exceptionally active user community. 3.1.1 Obtaining and installing R Installation files and instructions can be downloaded from the Comprehensive R Archive Network (CRAN). Go to the following link https://cran.r-project.org/ to download and install R. Pick an installation file for your operational system. Choose the “base” distribution of R (particularly if it is the first time you install R). Download the R installation file and open the file on your device. Follow the installation instructions. 3.1.2 Obtaining and installing RStudio Beginners will find it very hard to start using R because it has no Graphical User Interface (GUI). There are some GUIs which offer some of the functionality of R. RStudio makes R easier to use. It includes a code editor, debugging and visualization tools. Similar steps need to be followed to install RStudio. Go to https://www.rstudio.com/products/rstudio/download/ to download and install RStudio’s open source edition. On the download page, RStudio Desktop, Open Source License option should be selected. Pick an installation file for your platform. Follow the installation instruction on your local device. Figure 3.1: R Studio interface. The RStudio interface is structured by four compartments (see Fig. 3.1). The code editor is located in the upper left. Scripts that contain codes are displayed here. New scripts can be opened by clicking on the left most New button in the quick access tool bar (highlighted in green). Lines of code can be executed by clickinig on Run (highlighted in blue) or by pressing ctrl + enter on your keyboard. The output of scripts or lines of code that are executed is displayed in the window below the code editor: the console (bottom left). This part of the interface corresponds to the R software that were installed previously. When working in R, it is central to work with so-called objects (for instance vectors, dataframes or matrices). These objects are saved in the global environment that is displayed in the top right panel. Finally, the R software offers a broad range of powerful tools for visualisation purposes. Graphs or maps that are generated by scripts/codes, are displayed in the bottom right panel. 3.1.3 Getting started with R R manuals: http://cran.r-project.org/manuals.html Contributed documentation: http://cran.r-project.org/other-docs.html Quick-R: http://www.statmethods.net/index.html Stackoverflow R community: https://stackoverflow.com/questions/tagged/r 3.2 R packages When you download R, you get the basic R system which implements the R language. R becomes more useful with the large collection of packages that extend the basic functionality of it. R packages are developed by the R community. refer to: * tidyverse book (R for data science) * caret (broad range of statistical learning functions) * R spatial: https://rspatial.org/ (R packages for spatial data operations) The primary source for R packages is CRAN’s official website, where currently about 12,000 available packages are listed. For spatial applications, various packages are available. You can obtain information about the available packages directly on CRAN with the ‘available.packages()’ function. The function returns a matrix of details corresponding to packages currently available at one or more repositories. An easier way to browse the list of packages is using the Task Views link, which groups together packages related to a given topic. Packages come along with extensive documentation that is very helpful to understand and solve error messages. To access information on functions or packages, type “?[Package or Function name]” in the console. The information on the package and/or function can then be accessed in the bottom right panel under “Help” (see Fig. 3.1). In addition to that, the R documentation website (https://www.rdocumentation.org/) provides more extensive help and gives clear overviews on all functions comprised in a certain package. 3.3 GEE - google earth engine Google earth engine (GEE) provides a large range of remote sensing datasets for users. It allows to use the GEE code editor to run computations using the google servers. The high computational power of these servers enables users with limited computational caapacities to run complex calculations. A user account must be created to use the code editor. This step can take some time. Once the account is validated, scripts can be written in the code editor using the Javascript language. An extensive array of instructions and guides are available on the platform. Alternatively, the Python language can be used to interact with the data. The code editor interface is structured by three panels and a map viewer (see Fig. 3.2). The left panel is structured in “Scripts”, “Docs”, and “Assets”. Under “Scripts” users can organize and save the scripts they wrote for specific purposes. “Docs” provides further information on so-called “server-side” functions that can be used to manipulate the data. Finally, in “Assets” users can upload own spatial data in common formats such as shapefiles (.shp) or raster files (.tif). The middle panel contains the scripts that can be run by clicking on the “Run” button. The right panel is composed of three functionalities. The “Inspector” provides basic information on a pixel of a layer displayed in the map below. The information consists of longitude, latitude, and - if layers are loaded - values of the pixel. The “Console” is the place where certain commands expressed in the code are shown. The most common expressions shown here are print() commands or figures derived from the loaded data. Finally, the “Tasks” button shows all tasks that were formulated in the code/script and are to be submitted to the server for computation. Once a task is submitted, the user has to click on the “Run” button appearing in the “Tasks” section to submit the task to the server. In addition to that, the data catalog can be accessed via the search bar on the top of the page. Here, key information on the available datasets, origin, resolution and related publications can be found. Figure 3.2: Google Earth Engine code editor. The upload of assets in particular shapefiles with the area of interest (AOI) is often a key step to extract only the necessary information from a dataset. To upload a dataset, once has to right-click on “Assets” in the right panel and then on the red “New” button in the code editor interface (see Fig. 3.3). After selecting the file format of file, a second window opens. Figure 3.3: Select files and filetype to be uploaded as GEE assets. Here the source file needs to be selected from the respective folder and a folder in the GEE environment has to be selected under “Asset ID”. After that, the user right-clicks on “Uplaod” in the bottom right of the window (see Fig. 3.4). In the right panel under “Tasks” appears the uploading task that is executed. Once this process is finished, a tick mark appears and after refreshing the “Assets” panel on the left, the user can see the uploaded shapefile. Figure 3.4: Upload interface. 3.4 rgee - Extension to use google earth engine in R To generate the GSNmap, the user is going to use an R package to interact with the GEE environment. The rgee package enables users to interact with the GEE servers using the R language (Aybar, 2022). The package makes use of the Python language to interact with GEE. The package can be downloaded easily either directly from the GitHub repository or via CRAN. The prerequisite to have access to GEE is to create a user account on https://earthengine.google.com/. Once you have a GEE account, it is necessary to install the rgee package on your machine. After installing and loading the package to your session in R studio, it is necessary to install the so-called “Miniconda” commmand prompt which acts as a Python interpreter mediating between R and GEE. The ‘ee_install()’ function automatically downloads and install all the software that is needed. # install package install.packages(&quot;rgee&quot;) # load rgee package library(rgee) # install miniconda and other dependencies ee_install(py_env = &quot;rgee&quot;) Once the dependencies are installed, it is necessary to initialize rgee by providing the user credentials of the created GEE account. The ee_Initialize command must be run every time rgee is used. # Initialize Google Earth Engine! (you need to create a user account) ee_Initialize() In case problems arise during the installation, the following functions may be helpful to clear user credentials or clean the python environment created with the ee_install() command. In first place, it is recommendable to consult the rgee documentation website (https://r-spatial.github.io/rgee/articles/rgee01.html) to get more detailed information on the installation procedure and possibilities for problem solving. # Useful functions #ee_check() # check the dependencies that do not belong to R #ee_clean_credentials() # to remove the user credentials #ee_clean_pyenv() # Delete variables of the system References "],["digital-soil-mapping.html", "Chapter 4 Digital Soil Mapping 4.1 Principles 4.2 Environmental covariates 4.3 Machine learning techniques 4.4 Mapping of soil nutrients and associated soil attributes", " Chapter 4 Digital Soil Mapping 4.1 Principles Digital soil mapping (DSM) is a methodological framework to create soil attribute maps on the basis of the quantitative relationships between spatial soil databases and environmental covariates. The quantitative relations can be modelled by different statistical approaches, most of them considered machine learning techniques. Environmental covariates are spatially explicit proxies of soil-forming factors that are employed as predictors of the geographical distribution of soil properties. The methodology has evolved from the theories of soil genesis developed by Dokuchaev (1883) in his work the Russian Chernozems, which later were formalised by Jenny (1941) with the equation of the soil-forming factors. The conceptual equation of soil-forming factors has been updated by McBratney, Santos and Minasny (2003) as follow: \\[\\begin{equation} S = f\\left(s,c,o,r,p,a,n\\right) \\tag{4.1} \\end{equation}\\] Where \\(S\\) is the soil classes or attributes (to be modelled) as a function of “\\(s\\)” as other soil properties, “\\(c\\)” as climatic properties, “\\(o\\)” as organisms, including land cover and human activity, “\\(r\\)” as terrain attributes, “\\(p\\)” as parent material, “\\(a\\)” as soil age, and “\\(n\\)” as the geographic position. 4.2 Environmental covariates There is an constantly increasing range of global datasets that can be used as environmental covariates. Covariates usually provide information on the soil forming factors. However, they are always only an approximation to the reality in the field. The selection of covariates aims to give the most accurate picture of the reality and thus complement each other. In the case of climatic covariates for instance, useful covariates should not only cover the long-term mean annual temperature or precipitation over an climatic reference period (30 years) but also inform about seasonal patterns or even diurnal variability. Still, when selecting covariates one has to keep in mind that there is a trade-off between accurate representation of reality and overfitting the model used for modelling. 4.3 Machine learning techniques A broad range of modelling approaches coexist in order to establish quantitative relationships between environmental covariates and the target soil properties to be mapped. The plethora of methods cannot be listed here as it was summarised in multiple review papers (Khaledian and Miller, 2020; Lamichhane, Kumar and Wilson, 2019; Ma et al., 2019; Padarian, Minasny and McBratney, 2019; Wadoux, Minasny and McBratney, 2020). Traditionally, multiple linear regression models can be used to quantify the relationships which continues to be the most applied mapping method to map for instance soil organic carbon (Lamichhane, Kumar and Wilson, 2019). In addition to that, regression Kriging methods combine linear regressions and an stochastic interpolation of the regression residuals based on their spatial autocorrelation (Yigini et al., 2018). However, machine learning algorithms with more flexible assumptions, i.e. non-linear relationships, have become more and more popular as the mapping performance was substantially improved and the versatility of the algorithms can be detect more complex relationships. Among the most commonly used non-linear machine learning models is random forest (Breiman, 2001). The random forest algorithm splits a dataset into subsets and uses a random selection of covariates (predictors) to identify homogeneous groups. The procedure of classifying is repeated many times and in the end the prediction is averaged. Finally, quantile regression forests (QRF) derive from random forest models (Meinshausen, 2006). The benefit of QRF is the ability to predict not only the mean of the prediction but also to provide more information on the uncertainty and probability distribution. 4.4 Mapping of soil nutrients and associated soil attributes DSM has been used to produce maps of soil nutrients at regional to continental scales. For instance, Hengl et al. (2017) predicted 15 soil nutrients at a 250 m resolution in Africa using a random forest model (Wright, Ziegler and König, 2016). The soil nutrient observations were collected for topsoils at locations that were unevenly distributed over the continent and a set of spatially-explicit environmental covariates including soil properties. In 2021, the map resolution was increased to 30 x 30 m by using additional soil samples (Hengl et al., 2021). In Europe maps of chemical soil properties, including macronutrients like potassium and phosphorus, were mapped based on a gaussian process regression using the LUCAS soil database (Ballabio et al., 2019). Global efforts to map nutrients in a harmonised way are suffering of constraints due to limited availability of appropriate soil data. The country-driven approach of the GSP has therefore the potential to improve data availability through the country-driven approach as it uses largely unexplored soil data, a harmonised mapping approach combined with national expertise on the regional soil resources. Therefore, in this technical manual, we present a DSM framework to map soil nutrients and associated properties using soil observations with latitude and longitude coordinates (point-support) (Figure 4.1). Figure 4.1: Digital soil mapping approach for point-support data. Circles are the steps. References "],["step-1-soil-data-preparation.html", "Chapter 5 Step 1: soil data preparation 5.1 Format requirements of soil data 5.2 Preparatory steps 5.3 Data quality check 5.4 Calculation of pedo-transfer functions 5.5 Check for outliers 5.6 Harmonise soil layer depths 5.7 Save the results", " Chapter 5 Step 1: soil data preparation 5.1 Format requirements of soil data Soil data consist of measurement at a specific geographical location, time and soil depth. Therefore, it is necessary to arrange the data following the format shown in Table 5.1. Table 5.1: Example format of a database. Profile ID Horizon ID Lat Long Year Top Bottom Soil property Value Lab method 1 1_1 12.12346 1.123456 2018 0 20 SOC 3.4 W&amp;B 1 1_2 12.12346 1.123456 2018 20 40 SOC 2.1 W&amp;B 2 2_1 23.12346 2.123456 2019 0 30 SOC 2.9 W&amp;B Note: Profile ID = unique profile identifier, Horizon ID = unique layer identifier, Lat = latitude in decimal degrees, Long = longitude in decimal degrees, Year = sampling year, Top = upper limit of the layer in cm, Bottom = lower limit of the layer in cm, Soil property = name of the soil property, Value = numerical value of the measure, Lab method = name of the laboratory protocol used for measuring the soil property. Soil data usually require a pre-processing step to solve common issues such as, arranging the data format, fixing soil horizon depth consistency, detecting unusual soil property measurements, among others issues. Once the original dataset is clean and consistent, data harmonisation is needed to produce synthetic horizons (such as 0–30 cm layer), as well as to make compatible measurements from different lab methods. Horizon harmonisation will be done with the mass preserving spline function (Bishop, McBratney and Laslett, 1999; Malone et al., 2009) fitted to each individual soil profile, which requires more than a layer per profile. In this chapter, step-by-step instructions are given on how to carry out all these steps in RStudio. Instructions are given on how to: generate user-defined variables, set the working directory and load necessary packages, import national data to R Studio select useful columns perform a quality check of the data estimate bulk density using PTF harmonize soil layers (using splines) plot and save the formatted soil data Thus, the instructions also serve as a very basic introduction to the functioning of R and RStudio. Still, in case for further information there is a vast amount of websites that offer help and or information on R and RStudio. 5.2 Preparatory steps Now, let’s open RStudio. Whenever starting to work on a project or task, it is necessary to set the working directory (WD). The WD is the folder path that is used by R to save the output, for instance a plot or a table that was generated while working in R. Thus, the WD is central since it dictates where the files and calculations can be found afterwards. As it is so important, there are multiple ways of setting the WD. One option is to right click on ‘Session’ menu &gt; ‘Set working directory …’ and select either ‘To Source File Location’ (then the WD corresponds to the file path where the Script is saved to) or ‘Choose Directory…’. Then, the user can browse to the folder that should be the WD. In this manual we propose an alternative way that allows for more customization and flexibility since sometimes multiple WDs are needed to for instance save the final map in a different folder than the covariates. Therefore, we assign the file path that represents the WD file path to an R object. This is done by defining a character value (in this case the file path) on the right side of the arrow (&lt;-) and name the R object on the left side (wd) (see code). Once this is done we use the function ‘setwd()’ to set the WD to the file path that is specified in the object wd. Before working on specific tasks in R, it is necessary to load the packages that contain the functions that are going to be used. Before one can load packages, they need to be installed. There are many ways to do this, yet the most common way is to call the function install.packages(). For instance, to install the terra package, one has to write install.packages(“terra”). This installs the package from CRAN. However, there are a few exceptions where development versions of R packages are required. In these instances additional packages such as ‘devtools’ or ‘remotes’ are needed (see example in code below). These packages are then able to install packages from for instance GitHub repositories. # 0 - User-defined variables =================================================== wd &lt;- &#39;C:/Users/hp/Documents/GitHub/Digital-Soil-Mapping&#39; #wd &lt;- &quot;C:/GIT/Digital-Soil-Mapping&quot; # 1 - Set working directory and load necessary packages ======================== setwd(wd) # change the path accordingly library(tidyverse) # for data management and reshaping library(readxl) # for importing excel files library(mapview) # for seeing the profiles in a map library(sf) # to manage spatial data (shp vectors) # install.packages(&quot;devtools&quot;) # devtools::install_bitbucket(&quot;brendo1001/ithir/pkg&quot;) #install ithir package library(ithir) # for horizon harmonization The next step is to load the national soil data into R Studio. For that, it is recommendable to have the data in either Microsoft Excel format (.xlsx) or as comma separated value table (.csv). Then, the datasets can be loaded from the specified folder using the respective functions specified in the code below. It is noteworthy that in R datasets also need to be assigned to a user-defined variable in order to be saved in the “global environment”. After reading in the file, the package tidyverse comes into play. By using the select() and unique() functions, the user can select only the necessary columns from the table and ensure that no duplicates are included. At this point it may be necessary to rename certain columns, as shown for the Profile and Horizon ID columns in the code below. Finally, every time new datasets are loaded into R, it is recommendable to check the data. Using the summary() function, users can see the class of each variable (= column) and descriptive statistics (for numerical variables). Classes are ‘character’ (chr) for text, integer (int) for whole numbers, and numeric (num) for numeric variables. # 2 - Import national data ===================================================== # Save your national soil dataset in the data folder /01-Data as a .csv file or # as a .xlsx file ## 2.1 - for .xlsx files ------------------------------------------------------- # Import horizon data # hor &lt;- read_excel(&quot;01-Data/soil_data.xlsx&quot;, sheet = 2) # # Import site-level data # site &lt;- read_excel(&quot;01-Data/soil_data.xlsx&quot;, sheet = 1) ## 2.2 - for .csv files -------------------------------------------------------- # Import horizon data hor &lt;- read_csv(file = &quot;01-Data/soil_profile_data.csv&quot;) site &lt;- select(hor, id_prof, x, y, date) %&gt;% unique() hor &lt;- select(hor, id_prof, id_hor, top:cec) # change names of key columns names(site) names(site)[1] &lt;- &quot;ProfID&quot; names(hor) names(hor)[1] &lt;- &quot;ProfID&quot; names(hor)[2] &lt;- &quot;HorID&quot; # scan the data summary(site) summary(hor) The selection of useful columns is very important since it ensures that users keep a good overview and a clean environment. Using the select() function, it is also possible to rename the variables right away (see code below). # 3 - select useful columns ==================================================== ## 3.1 - select columns -------------------------------------------------------- hor &lt;- select(hor, ProfID, HorID, top, bottom, ph=ph_h2o, k, soc, clay, bd, cec) # the variable ph_h2o was renamed as ph 5.3 Data quality check Datasets need to be checked for their quality as especially manually entered data is prone to mistakes such as typos or duplicates. A thorough quality check ensures that: all profiles have reasonable coordinates (within the area of interest); there are no duplicated profiles; and the depth logic within a profile is not violated. To check the first point, the dataframe needs to be converted into a spatial object using the st_as_sf() function of the sf package. It is necessary to indicate the columns that contains latitude and longitude, as well as a coordinate reference system (CRS). We recommend WGS84 which corresponds to an EPSG code of 4326. However, locally more appropriate CRS can be found on the following website: https://epsg.io/. The mapview() command (from mapview package) offers the possibility to visualize the profile locations in an interactive map. Finally, the filter() function can be used to remove rows that contain profiles with wrong locations. # 4 - Quality check ============================================================ ## 4.1 - Check locations ------------------------------------------------------- # https://epsg.io/6204 site %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) %&gt;% # convert to spatial object mapview(zcol = &quot;ProfID&quot;, cex = 3, lwd = 0.1) # visualise in an interactive map # profile 2823 is wrongly located, so let&#39;s remove it site &lt;- filter(site, ProfID != 2823) To visualize the profile locations, the soil data table was converted into a shapefile. Still, to check whether the database complies with the depth logic within each profile, it is necessary to convert the data table into a so-called soil profile collection that allows for very specific operations. These operations were bundled in the package aqp (AQP = Algorithms for Quantitative Pedology) (Beaudette, Roudier and O’Geen, 2013). With the first lines of code below, the dataset is converted into a soil profile collection and profiles and horizon tables are joined based on the site information. Now the profile collection can be visualised for any soil property. In this case, only the first 20 profiles are selected for the cation exchange capacity (CEC). Using the checkHzDepthLogic() function, users can assess that all profiles do not have gaps or overlaps of neighbouring horizons. If there are, they can be selected and checked through the Profile ID. In the following step, only profiles with valid horizon logic are selected. Finally, the soil profile collection is re-converted to a dataframe. With this, the quality check is finished. ## 4.2 - Convert data into a Soil Profile Collection --------------------------- depths(hor) &lt;- ProfID ~ top + bottom hor@site$ProfID &lt;- as.numeric(hor@site$ProfID) site(hor) &lt;- left_join(site(hor), site) profiles &lt;- hor profiles ## 4.3 - plot first 20 profiles using CEC as color ------------------------------ plotSPC(x = profiles[1:20], name = &quot;cec&quot;, color = &quot;cec&quot;, name.style = &quot;center-center&quot;) ## 4.4 - check data integrity -------------------------------------------------- # A valid profile is TRUE if all of the following criteria are false: # + depthLogic : boolean, errors related to depth logic # + sameDepth : boolean, errors related to same top/bottom depths # + missingDepth : boolean, NA in top / bottom depths # + overlapOrGap : boolean, gaps or overlap in adjacent horizons aqp::checkHzDepthLogic(profiles) # visualize some of these profiles by the pid subset(profiles, grepl(6566, ProfID, ignore.case = TRUE)) subset(profiles, grepl(6915, ProfID, ignore.case = TRUE)) subset(profiles, grepl(7726, ProfID, ignore.case = TRUE)) ## 4.5 - keep only valid profiles ---------------------------------------------- clean_prof &lt;- HzDepthLogicSubset(profiles) metadata(clean_prof)$removed.profiles # write_rds(clean_prof, &quot;01-Data/soilProfileCollection.rds&quot;) ## 4.6 convert soilProfileCollection to a table -------------------------------- dat &lt;- left_join(clean_prof@site, clean_prof@horizons) dat &lt;- select(dat, ProfID, HorID, x, y, date, top, bottom, ph:cec ) The soil data table is now revised and a sound and consistent quality is ensured. Thus, the available data can be used in the following to perform calculations in order to account for missing soil properties for instance. In the following example, a set of pedotransfer functions (PTF) will be calculated based on the organic matter (OM) content in order to estimate bulk density (BD) for missing observations. 5.4 Calculation of pedo-transfer functions In the cases of single-layer samples, which is common in sampling for nutrient determination, a locally calibrated pedotransfer function (PTF) should be applied. PTF will be also required to harmonise the laboratory methods. Experts from GLOSOLAN will provide advice in this regard. Therefore, a customised function is introduced to our working environment. Users can write their own functions in R. This is often necessary when existing functions need to be customised or very specific calculations need to be performed. Functions greatly increase the efficiency of our code. For further information, it is recommendable to consult online resources on the topic (e.g. https://hbctraining.github.io/Intro-to-R/lessons/03_introR-functions-and-arguments.html). The function ‘estimateBD’ below calculates various PTFs that estimate BD. Which equation is used is determined by the user that has to choose one of the methods and also specify the SOC value of the respective horizon. The SOC values is first converted to OM by using the conversion factor of 1.724 and then inserted in the respective PTF. The return() command tells R which value to output. # 5 - Estimate BD using pedotransfer functions ================================= # create the function with all PTF estimateBD &lt;- function(SOC=NULL, method=NULL){ OM &lt;- SOC * 1.724 if(method==&quot;Saini1996&quot;){BD &lt;- 1.62 - 0.06 * OM} if(method==&quot;Drew1973&quot;){BD &lt;- 1 / (0.6268 + 0.0361 * OM)} if(method==&quot;Jeffrey1979&quot;){BD &lt;- 1.482 - 0.6786 * (log(OM))} if(method==&quot;Grigal1989&quot;){BD &lt;- 0.669 + 0.941 * exp(1)^(-0.06 * OM)} if(method==&quot;Adams1973&quot;){BD &lt;- 100 / (OM /0.244 + (100 - OM)/2.65)} if(method==&quot;Honeyset_Ratkowsky1989&quot;){BD &lt;- 1/(0.564 + 0.0556 * OM)} return(BD) } To apply the ‘estimateBD’ function, first a test dataframe is created that includes the SOC values from the cleaned profile table as well as the respective existing BD measurements. The rows without values in one of the columns are excluded using the na.omit() function since we want to first evaluate the difference between estimated BDs and measured BDs. Now, the test dataframe is complemented by the estimated BDs derived from the PTFs for each method. To add new columns to an existing dataframe one has to write on the left-hand side of the arrow the name of the existing dataframe object (in this case BD_test), the dollar sign ($), and the name of the new column. Here, the names are given according to the used BD PTF. ## 5.1 - Select a pedotransfer function ---------------------------------------- # create a vector of BD values to test the best fitting pedotransfer function BD_test &lt;- tibble(SOC = clean_prof@horizons$soc, BD_test = clean_prof@horizons$bd) BD_test &lt;- na.omit(BD_test) ## 5.2 - Estimate BLD for a subset using the pedotransfer functions ------------ BD_test$Saini &lt;- estimateBD(BD_test$SOC, method=&quot;Saini1996&quot;) BD_test$Drew &lt;- estimateBD(BD_test$SOC, method=&quot;Drew1973&quot;) BD_test$Jeffrey &lt;- estimateBD(BD_test$SOC, method=&quot;Jeffrey1979&quot;) BD_test$Grigal &lt;- estimateBD(BD_test$SOC, method=&quot;Grigal1989&quot;) BD_test$Adams &lt;- estimateBD(BD_test$SOC, method=&quot;Adams1973&quot;) BD_test$Honeyset_Ratkowsky &lt;- estimateBD(BD_test$SOC, method=&quot;Honeyset_Ratkowsky1989&quot;) The calculated BDs can now be compared using the summary() function. However, a faster and more accessible approach is to plot the different bulk densities for comparison. In case you are not familiar with the plot() function and its respective commands, it is recommendable to check one of the many online learning resources such as https://intro2r.com/simple-base-r-plots.html. The plot shows us both measured and estimated BD values as differently coloured lines (see Fig ???). ## 5.3 Compare results --------------------------------------------------------- # Observed values: summary(BD_test$BD_test) # Predicted values: summary(BD_test$Saini) summary(BD_test$Drew) summary(BD_test$Jeffrey) summary(BD_test$Grigal) summary(BD_test$Adams) summary(BD_test$Honeyset_Ratkowsky) # Compare data distributions for observed and predicted BLD plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,5), lwd=2, main=&quot;Bulk Density Pedotransfer Functions&quot;) lines(density(BD_test$Saini),col=&quot;green&quot;, lwd=2) lines(density(BD_test$Drew),col=&quot;red&quot;, lwd=2) lines(density(BD_test$Jeffrey),col=&quot;cyan&quot;, lwd=2) lines(density(BD_test$Grigal),col=&quot;orange&quot;, lwd=2) lines(density(BD_test$Adams),col=&quot;magenta&quot;, lwd=2) lines(density(BD_test$Honeyset_Ratkowsky),col=&quot;blue&quot;, lwd=2) legend(&quot;topleft&quot;, legend = c(&quot;Original&quot;, &quot;Saini&quot;, &quot;Drew&quot;, &quot;Jeffrey&quot;, &quot;Grigal&quot;, &quot;Adams&quot;, &quot;Honeyset_Ratkowsky&quot;), fill=c(&quot;black&quot;, &quot;green&quot;, &quot;red&quot;, &quot;cyan&quot;, &quot;orange&quot;,&quot;magenta&quot;, &quot;blue&quot;)) # Plot the Selected function again plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,3.5), lwd=2, main=&quot;Bulk Density Selected Function&quot;) lines(density(BD_test$Honeyset_Ratkowsky),col=&quot;blue&quot;, lwd=2) legend(&quot;topleft&quot;,legend = c(&quot;Original&quot;, &quot;Honeyset_Ratkowsky&quot;), fill=c(&quot;black&quot;, &quot;blue&quot;)) The PTF to be chosen for estimating the BD of the missing horizons should be the closest to the measured BD values. Once, the appropriate PTF was chosen, the ‘estimateBD’ function is applied in the dataframe ‘dat’ that was created at the end of the quality check. Here, new bd values are estimated for the rows in which the column ‘bd’ has missing values. Finally, a plot is generated to visualize the gap-filled bulk density values. ## 5.4 Estimate BD for the missing horizons ------------------------------------ dat$bd[is.na(dat$bd)] &lt;- estimateBD(dat[is.na(dat$bd),]$soc, method=&quot;Honeyset_Ratkowsky1989&quot;) # Explore the results summary(dat$bd) plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,3.5), lwd=2, main=&quot;Bulk Density Gap-Filling&quot;) lines(density(dat$bd, na.rm = TRUE), col=&quot;green&quot;, lwd=2) legend(&quot;topleft&quot;,legend = c(&quot;Original&quot;, &quot;Original+Estimated&quot;), fill=c(&quot;black&quot;, &quot;green&quot;)) 5.5 Check for outliers Unrealistically high or low values can have considerable impact on the statistical analysis and thus it is key to identify and carefully check those values in order to get valid results and eliminate potential bias. Again, the summary() function is apt to show general descriptive statistics such as maxima or minima. Based on this assessment, more detailed views of the suspicious values can be obtained by filtering values above or below a certain threshold as done in the code below for soil organic carbon (SOC) values above 10 percent. If such values don’t belong to soil types that would justify such exceptionally high SOC values, e.g. organic soils (Histosols), these rows can be removed based on the profile ID. The same process should be repeated for all soil properties. Such evaluation can also be conducted visually for several properties at the same time using the ‘tidyverse’ and ‘ggplot’ package that allows to plot boxplots for several soil properties at the same time. To get more information on tidyverse, please follow this link: https://r4ds.had.co.nz/. For a comprehensive overview of the functionalities of ggplot, a more sophisticated way of plotting, this book provides a good overview: http://www.cookbook-r.com/Graphs/. ## 5.5 - Explore outliers ------------------------------------------------------ # Outliers should be carefully explored and compared with literature values. # Only if it is clear that outliers represent impossible or highly unlikely # values, they should be removed as errors. # # Carbon content higher than 15% is only typical for organic soil (histosols) # We will remove all atypically high SOC as outliers summary(dat$soc) na.omit(dat$ProfID[dat$soc &gt; 10]) dat &lt;- dat[dat$ProfID != 6915,] dat &lt;- dat[dat$ProfID != 7726,] # Explore bulk density data, identify outliers # remove layers with Bulk Density &lt; 1 g/cm^3 low_bd_profiles &lt;- na.omit(dat$ProfID[dat$bd&lt;1]) dat &lt;- dat[!(dat$ProfID %in% low_bd_profiles),] # Explore data, identify outliers x &lt;- pivot_longer(dat, cols = ph:cec, values_to = &quot;value&quot;, names_to = &quot;soil_property&quot;) x &lt;- na.omit(x) ggplot(x, aes(x = soil_property, y = value, fill = soil_property)) + geom_boxplot() + facet_wrap(~soil_property, scales = &quot;free&quot;) 5.6 Harmonise soil layer depths The last step towards a soil data table that can be used for mapping, is to harmonize the soil depth layers to 0-30 cm (or 30-60, or 60-100 cm respectively). This is necessary since we want to produce maps that cover exactly those depths and do not differ across soil profile locations. Thus, the relevant columns are selected from the dataframe, target soil properties, and upper and lower limit of the harmonised soil layer are specified (in depths). In the following a new dataframe ‘d’ is created in which the standard depth layers are stored. The code below shows a for loop that calculates the values for the standard depth for each target soil property automatically using the ea_spline function of the ‘ithir’ package. The spline functions are also explained in Format requirements of soil data. # 6 - Harmonize soil layers ==================================================== ## 6.1 - Set target soil properties and depths --------------------------------- names(dat) dat &lt;- select(dat, ProfID, HorID, x, y, top, bottom, ph, k, soc, clay, bd, cec) target &lt;- c(&quot;ph&quot;, &quot;k&quot;, &quot;soc&quot;, &quot;clay&quot;, &quot;bd&quot;, &quot;cec&quot;) depths &lt;- t(c(0,30)) ## 6.2 - Create standard layers ------------------------------------------------ d &lt;- unique(select(dat, ProfID, x, y)) for (i in seq_along(target)) { vlow &lt;- min(dat[,target[i]][[1]], na.rm = TRUE) vhigh &lt;- max(dat[,target[i]][[1]], na.rm = TRUE) o &lt;- dat[,c(&quot;ProfID&quot;, &quot;top&quot;, &quot;bottom&quot;,target[i])] %&gt;% na.omit() %&gt;% as.data.frame(stringsAsFactors = FALSE) x &lt;- ithir::ea_spline(obj = o, var.name = target[i], d = depths, vlow = vlow[[1]], vhigh = vhigh[[1]])$harmonised x[x==-9999] &lt;- NA x &lt;- x %&gt;% as_tibble() %&gt;% select(-`soil depth`) names(x) &lt;- c(&quot;ProfID&quot;,paste0(target[i],c(&quot;_0_30&quot;,&quot;_30_60&quot;,&quot;_60_100&quot;))) d &lt;- d %&gt;% left_join(x, by = &quot;ProfID&quot; ) } d 5.7 Save the results Before finalising the soil data preparation, it is recommendable to check again visually if the calculations were conducted correctly. Again, the combination of tidyverse and ggplot functions provides high efficiency and versatility to visualise figures with the desired soil properties. At last, the write_csv() function is used to save the dataframe as a .csv file in the Outputs folder (02-Outputs). With this, the soil data preparation is finalised. # 7 - Plot and save results =================================================== x &lt;- pivot_longer(d, cols = ph_0_30:cec_0_30, values_to = &quot;value&quot;, names_sep = &quot;_&quot;, names_to = c(&quot;soil_property&quot;, &quot;top&quot;, &quot;bottom&quot;)) x &lt;- mutate(x, depth = paste(top, &quot;-&quot; , bottom)) x &lt;- na.omit(x) ggplot(x, aes(x = depth, y = value, fill = soil_property)) + geom_boxplot() + facet_wrap(~soil_property, scales = &quot;free&quot;) # remove BD and CF # d &lt;- select(d, ProfID:y, soc_0_30:ocs_60_100) # save data write_csv(d, &quot;02-Outputs/spline_soil_profile.csv&quot;) References "],["step-2-download-environmental-covariates.html", "Chapter 6 Step 2: download environmental covariates 6.1 Environmental covariates 6.2 Reducing collinearity in environmental covariates 6.3 Merging soil data and environmental covariates", " Chapter 6 Step 2: download environmental covariates 6.1 Environmental covariates The SCORPAN equation (Eq. \\(\\ref{eq:scorpan}\\)) refers to the soil-forming factors that determine the spatial variation of soils. However, these factors cannot be measured directly. Instead, proxies of these soil forming factors are used. One essential characteristic of the environmental covariates is that they are spatially explicit, covering the whole study area. Table 2 shows a summary of the environmental covariates that can be implemented under the DSM framework. (#tab:table6.1)List of covariates. Description Code Resolution m Temperature Mean air temperature (annual) bio1 1000 Mean daily temperature of warmest month bio5 1000 Mean daily temperature of coldest month bio6 1000 Precipitation Mean annual precipitation bio12 1000 Mean precipitation of wettest month bio13 1000 Mean precipitation of driest month bio14 1000 Mean monthly precipitation of wettest quarter bio16 1000 Mean monthly precipitation of driest quarter bio17 1000 Potential evapotranspiration (PET) Mean monthly PET pet_penman_mean 1000 Minimum monthly PET pet_penman_min 1000 Range monthly PET pet_penman_range 1000 Maximum monthly PET pet_penman_max 1000 Wind Minimum monthly wind speed sfcWind_min 1000 Maximum monthly wind speed sfcWind_max 1000 Range monthly wind speed sfcWind_range 1000 Growing season Number of days with mean daily air ngd10 1000 temperature above 10 degrees Celsius Vegetation indices (NDVI (MOD13Q1)) Mean March-May from 2000-2022 ndvi_030405_mean 250 Mean June-August from 2000-2022 ndvi_060708_mean 250 Mean September-November from 2000-2022 ndvi_091011_mean 250 Mean December-February from 2000-2022 ndvi_120102_mean 250 Standard deviation March-May (2000-2022) ndvi_030405_sd 250 Standard deviation June-August (2000-2022) ndvi_060708_sd 250 Standard deviation Sept.-Nov. (2000-2022) ndvi_091011_sd 250 Standard deviation Dec.-Feb. (2000-2022) ndvi_120102_sd 250 Fraction of photosynthetically active radiation (FPAR) (MOD15A2H) Mean March-May from 2000-2022 fpar_030405_mean 500 Mean June-August from 2000-2022 fpar_060708_mean 500 Mean September-November from 2000-2022 fpar_091011_mean 500 Mean December-February from 2000-2022 fpar_120102_mean 500 Standard deviation March-May (2000-2022) fpar_030405_sd 500 Standard deviation June-August (2000-2022) fpar_060708_sd 500 Standard deviation Sept.-Nov. (2000-2022) fpar_091011_sd 500 Standard deviation Dec.-Feb. (2000-2022) fpar_120102_sd 500 Land surface temperature day (LSTD) (MOD11A2) Mean March-May from 2000-2022 lstd_030405_mean 1000 Mean June-August from 2000-2022 lstd_060708_mean 1000 Mean September-November from 2000-2022 lstd_091011_mean 1000 Mean December-February from 2000-2022 lstd_120102_mean 1000 Standard deviation March-May (2000-2022) lstd_030405_sd 1000 Standard deviation June-August (2000-2022) lstd_060708_sd 1000 Standard deviation Sept.-Nov. (2000-2022) lstd_091011_sd 1000 Standard deviation Dec.-Feb. (2000-2022) lstd_120102_sd 1000 Normalised difference between LST day and LST night (MOD11A2) Mean March-May from 2000-2022 ndlstd_030405_mean 1000 Mean June-August from 2000-2022 ndlstd_060708_mean 1000 Mean September-November from 2000-2022 ndlstd_091011_mean 1000 Mean December-February from 2000-2022 ndlstd_120102_mean 1000 Standard deviation March-May (2000-2022) ndlstd_030405_sd 1000 Standard deviation June-August (2000-2022) ndlstd_060708_sd 1000 Standard deviation Sept.-Nov. (2000-2022) ndlstd_091011_sd 1000 Standard deviation Dec.-Feb. (2000-2022) ndlstd_120102_sd 1000 Short-wave Infrared (SWIR) black-sky albedo for shortwave broadband (MCD43A3) Mean June-August from 2000-2022 swir_060708_mean 5– MODIS snow cover (MOD10A1) Mean snow cover mean_snow_cover 500 Land cover dynamic world 10m near real-time land use/land cover (LULC) dataset Mean estimated probability of complete coverage by trees trees 250 Mean estimated probability of complete coverage shrub_and_scrub 250 by shrub and scrub Mean estimated probability of complete coverage flooded_vegetation 250 by flooded vegetation Mean estimated probability of complete coverage grass 250 by grasses Mean estimated probability of complete coverage crop 250 by bare Terrain Profile curvature curvature 250 Downslope curvature downslopecurvature 250 Upslope curvature upslopecurvature 250 Deviation from mean value dvm 250 deviation from mean value dvm2 250 Elevation elevation 250 Melton ruggedness number mrn 250 Negative openness neg-openness 250 Positive openness pos-openness 250 Slope slope 250 Topographic position index tpi 250 Terrain wetness index twi 250 Multiresolution of valley bottom flatness vbf 250 #Empty environment and cache rm(list = ls()); gc() # Content of this script ======================================================= # The goal of this script is to organise to clip and dowload the covariates and # it includes the following steps: # # 0 -User-defined variables # 1 - Set working directory and load necessary packages # 2 - Import shapefile # 3 - Overview of covariates # 4 - Initialize GEE # 5 - Upload shapefile to GEE OR use uploaded UN borders # 6 - Clip and download the covariates #_______________________________________________________________________________ # 0 - User-defined variables =================================================== # Working directory #wd &lt;- &#39;C:/Users/luottoi/Documents/GitHub/Digital-Soil-Mapping&#39; wd &lt;- &#39;C:/GIT/Digital-Soil-Mapping&#39; # Output covariate folder #output_dir &lt;-&#39;&#39; output_dir &lt;-&#39;01-Data/covs/&#39; # Area of interest: either own shapefile or 3-digit ISO code to extract from # UN 2020 boundaries AOI &lt;- &#39;01-Data/AOI_Arg.shp&#39; # AOI &lt;- &#39;MKD&#39; # Resolution and projection res = 250 crs = &quot;EPSG:4326&quot; #_______________________________________________________________________________ # 1 - Set working directory and load necessary packages ======================= # Set working directory setwd(wd) #load libraries library(raster) library(terra) library(sf) library(rgee) # 2 - Import shapefile ========================================================= AOI &lt;- read_sf(AOI) # convert AOI to a box polygon AOI &lt;- st_as_sfc(st_bbox(AOI)) AOI &lt;- st_as_sf(AOI) # 3 - Overview of covariates =================================================== # CLIMATIC VARIABLES from CHELSA # VEGETATION INDICES, FPAR and LAND SURFACE TEMPERATURE from MODIS # LAND COVER LAYERS from Dynamic World 10m near-real-time (NRT) # TERRAINE attributes from OpenLandMap # for more information about the single covariates: open covariates.xslx in the # training material folder # 4 - Initialize GEE =========================================================== ee_Initialize() # 5 - Upload shapefile to GEE OR use uploaded UN borders ======================= ## 5.1 Convert shp to gee geometry --------------------------------------------- #region &lt;- sf_as_ee(AOI) #region = region$geometry() ## 5.2 Extract from UN 2020 map using ISO code --------------------------------- region &lt;-ee$FeatureCollection(&quot;projects/digital-soil-mapping-gsp-fao/assets/UN_BORDERS/BNDA_CTY&quot;)%&gt;% ee$FeatureCollection$filterMetadata(&#39;ISO3CD&#39;, &#39;equals&#39;, AOI) region = region$geometry() # AOI_shp &lt;-ee_as_sf(region) # AOI_shp &lt;- st_collection_extract(AOI_shp, &quot;POLYGON&quot;) # write_sf(AOI_shp, paste0(&#39;01-Data/&#39;,AOI,&#39;.shp&#39;)) # aoi &lt;- vect(AOI_shp) # 6 - Clip and download covariates ======================================== # Obtain list of climatic variables assetname&lt;- rbind(ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP&quot;)) assetname$num &lt;- 1:nrow(assetname) # Loop over the names of assets to clip and dowload the covariates for (i in unique(assetname$ID)){ #Extract filename filename &lt;- sub(&#39;.*\\\\/&#39;, &#39;&#39;, i) #Clip image to the extent of the AOI image &lt;- ee$Image(i) %&gt;% ee$Image$clip(region)%&gt;% ee$Image$toFloat() # Resample to target resolution image = image$resample(&#39;bilinear&#39;)$reproject( crs= crs, scale= res) #Export clipped covariate as raster raster &lt;- ee_as_raster( image = image, scale= res, region = region, via = &quot;drive&quot;, maxPixels = 1e+12 ) plot(raster) num &lt;- assetname[assetname$ID == i, &#39;num&#39;] writeRaster(raster, paste0(output_dir,filename, &#39;.tif&#39;), overwrite=T) print(paste(filename, &#39;exported successfully - Covariate&#39;,num, &#39;out of 68&#39;)) } Apart from the environmental covariates mentioned in Table \\(\\ref{tab:covs}\\), other types of maps could also be included, such as Global Surface Water Mapping Layers and Water Soil Erosion from the Joint Research Centre (JRC). At national level there may be very significant covariates that could complement or replace the covariates of Table \\(\\ref{tab:covs}\\). Since environmental covariates are available at different resolutions and coordinate reference systems (CRS), they have to be harmonised at a common resolution and CRS. The target resolution in GSNmap is 250 m x 250 m, therefore, all covariates were aggregated (from higher to lower resolution) or disaggregated (from lower to higher resolution) to 250 m. This process involved a raster resampling method, which is usually implemented by a bilinear approach for continuous covariates, and by the nearest-neighbour approach for categorical covariates (not included in the current list). Note that the target resolution of GSNmap has been set at 250 m, which can be considered a moderate resolution for a global layer. However, those countries that require a higher resolution are free to develop higher resolution maps and aggregate the resulting maps to the target resolution of GSNmap for submission. 6.2 Reducing collinearity in environmental covariates Multicollinearity is usually present in remote sensing data and terrain attributes. While this was an issue for multiple linear regression models, current models such as random forest can deal with high dimensionality. However, the main reasons to reduce the number of environmental covariates are that a model with fewer predictors can be interpreted more easily, thus extracting new knowledge, redundant information increasing the computational demand, and improve prediction results (Behrens et al., 2014). Covariate selection can be done by supervised or unsupervised methods (Behrens et al., 2010). Supervised methods work on the basis of prediction results, hence they are based on a given dataset. For instance, recursive feature elimination (RFE) in caret R package (Kuhn, 2022) provides a tool for selecting covariates according to their predicting contribution. Instead, unsupervised methods are used to reduce the dimensionality of the dataset by removing redundant information without taking into account a particular target variable. Principal component analysis is one of the most widely used for this purpose, however, it does not ensure that specific discriminant features are kept within the main factors (Behrens et al., 2014). Another drawback of this technique is that model interpretation can be reduced when using factors instead of the original covariates. 6.3 Merging soil data and environmental covariates A calibration dataset consists of soil observations and a matrix of predictors, where each row is a soil observation paired with the values of the corresponding covariates for the given spatial location. Some common issues and solution when merging soil observations and covariates are: Mismatch of coordinate reference system (CRS): it requires to convert the CRS of point data to the raster or polygon covariate CRS. Categorical covariates: some covariates may be categorical, such as land use/cover, legacy soil maps or geological maps. A common problem in this case is that some classes may not be sampled with any soil observation, causing an error when using the layer for prediction, since the model cannot predict over a class that was not part of the model calibration step. Also, because of the cross-validation procedure, it is advised to have, at least, three soil samples per class for the same reason. References "],["step-3-mapping-continuous-soil-properties.html", "Chapter 7 Step 3: Mapping continuous soil properties 7.1 Setting up repeated k-fold cross validation 7.2 Model calibration 7.3 Predicting soil attributes", " Chapter 7 Step 3: Mapping continuous soil properties 7.1 Setting up repeated k-fold cross validation Cross validation is one of the most used methods in DSM for assessing the overall accuracy of the resulting maps (Step 8, Figure 3). Since this is implemented along with the model calibration step, we explain the process at this stage. Cross validation consists of randomly splitting the input data into a training set and a testing set. However, a unique testing dataset can bias the overall accuracy. Therefore, k-fold cross validation randomly splits the data into k parts, using 1/k part of it for testing and k-1/k part for training the model. In order to make the final model more robust in terms of parameter estimations, we include repetitions of this process. The final approach is called repeated k-fold cross-validation, where k will be equal to ten in this process. A graphical representation of the 10-fold cross validation is shown in Figure \\(\\ref{fig:cv}\\). Note that green balls represent the samples belonging to the testing set and yellow balls are samples of the training set. Each row is a splitting step of the 10-folds, while each block (repetitions) represent the repetition step. Step 5 in Figure 3 represents the repeated cross-validation, but note that after each single splitting step (the rows in Figure 4) the training data go to model calibration, which will be explained in Step 6 (next Section), and the testing data will be used with the calibrated model to produce the residuals (Step 8, Section 2.2.8). Repeated cross validation has been nicely implemented in the caret R package (Kuhn, 2022), along with several calibration methods. 7.2 Model calibration The model calibration step involves the use of a statistical model to find the relations between soil observations and environmental covariates. One of the most widely used models in DSM is random forest (Breiman, 2001). Random forest is considered a machine learning method which belongs to the decision-tree type of model. Random forest creates an ensemble of trees using a random selection of covariate. The prediction of a single tree is made based on the observed samples mean in the leaf. The random forest prediction is made by taking the average of the predictions of the single trees. The size of the number of covariates at each tree (mtry) can be fine-tuned before calibrating the model. Quantile regression forests (QRF, Meinshausen (2006)) are a generalisation of the random forest models, capable of not only predicting the conditional mean, but also the conditional probability density function. This feature allows one to estimate the standard deviation of the prediction, as well as the likelihood of the target variable falling below a given threshold. In a context where a minimum level of a soil nutrient concentration may be decisive for improving the crop yield, this feature can play an important role for the GSNmap initiative. Model calibration will be implemented using the caret package (Kuhn, 2022). While we suggest to use QRF, caret provides a large set of models https://topepo.github.io/caret/available- models.html#) that might perform better in specific cases. In this regard, it is up to the user to implement a different model, ensuring the product specifications (Section Product Specifications). 7.3 Predicting soil attributes After calibrating the model, caret will select the best set of parameters and will fit the model using the whole dataset. Then, the final model can be used to predict the target soil properties. The process uses the model and the values of the covariates at target locations. This is generally done by using the same input covariates as a multilayer raster format, ensuring that the names of the layers are the same as the covariates in the calibration dataset. In this step we will predict the conditional mean and conditional standard deviation at each raster cell. References "],["step-4-uncertainty-assessment.html", "Chapter 8 Step 4: uncertainty assessment 8.1 Introduction", " Chapter 8 Step 4: uncertainty assessment 8.1 Introduction Accuracy assessment is an essential step in digital soil mapping. One aspect of the accuracy assessment has been done in Step 7 by predicting the standard deviation of the prediction, which shows the spatial pattern of the uncertainty. Another aspect of the uncertainty is the estimation of the overall accuracy to measure the model performance. This will be measured using the model residuals generated by caret during the repeated cross validation step. The residuals produced by caret consist of tabular data with observed and predicted values of the target soil property. They can be used to estimate different accuracy statistics. Wadoux, Walvoort and Brus (2022) have reviewed and evaluated many of them. While they concluded that there is not a single accuracy statistic that can explain all aspect of map quality, they recommended the following: mean prediction error (ME), that estimates the prediction bias; mean absolute prediction error (MAE) and root mean squared prediction error (RMSE) to estimate the magnitude of the errors; and model efficiency coefficient (MEC) (Janssen and Heuberger, 1995) as an estimator of the proportion of variance explained by the model. While solar diagrams (Wadoux, Walvoort and Brus, 2022) are desired, we propose to produce a scatterplot of the observed vs predicted values maintaining the same range and scale for the X and Y axes. Finally, note that accuracy assessment has been discussed in Wadoux et al. (2021), since the spatial distribution of soil samples might constrain the validity of the accuracy statistics. This is especially true in cases where the spatial distribution of observations is clustered. The authors recommended creating a kriging map of residuals before using them for assessing the map quality. References "],["compendium-of-r-scripts.html", "Chapter 9 Compendium of R scripts 9.1 Data preparation 9.2 Download environmental covariates 9.3 Modelling, validation and prediction using soil data with coordinates 9.4 Annex A: R scripts for extra functions", " Chapter 9 Compendium of R scripts This chapter contains the complete list of R scripts to run the process of mapping soil nutrient and associated soil properties. 9.1 Data preparation # # Digital Soil Mapping # Soil Profile Data # Cleaning and Processing # # GSP-Secretariat # Contact: Isabel.Luotto@fao.org # Marcos.Angelini@fao.org #_______________________________________________________________________________ #Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================================= # The goal of this script is to organise the soil data for mapping, including: # # 0 - User-defined variables # 1 - Set working directory and load necessary packages # 2 - Import national data # 3 - Select useful columns # 4 - Quality check # 5 - Estimate BD using pedotransfer function # 6 - Harmonize soil layers # 7 - Plot and save results #_______________________________________________________________________________ # 0 - User-defined variables =================================================== #wd &lt;- &#39;C:/Users/hp/Documents/GitHub/Digital-Soil-Mapping&#39; wd &lt;- &quot;C:/GIT/Digital-Soil-Mapping&quot; # 1 - Set working directory and load necessary packages ======================== setwd(wd) # change the path accordingly library(tidyverse) # for data management and reshaping library(readxl) # for importing excel files library(mapview) # for seeing the profiles in a map library(sf) # to manage spatial data (shp vectors) library(aqp) # for soil profile data # install.packages(&quot;devtools&quot;) # devtools::install_bitbucket(&quot;brendo1001/ithir/pkg&quot;) #install ithir package library(ithir) # for horizon harmonization # 2 - Import national data ===================================================== # Save your national soil dataset in the data folder /01-Data as a .csv file or # as a .xlsx file ## 2.1 - for .xlsx files ------------------------------------------------------- # Import horizon data # hor &lt;- read_excel(&quot;01-Data/soil_data.xlsx&quot;, sheet = 2) # # Import site-level data # site &lt;- read_excel(&quot;01-Data/soil_data.xlsx&quot;, sheet = 1) ## 2.2 - for .csv files -------------------------------------------------------- # Import horizon data hor &lt;- read_csv(file = &quot;01-Data/soil_profile_data.csv&quot;) site &lt;- select(hor, id_prof, x, y, date) %&gt;% unique() hor &lt;- select(hor, id_prof, id_hor, top:cec) # change names of key columns names(site) names(site)[1] &lt;- &quot;ProfID&quot; names(hor) names(hor)[1] &lt;- &quot;ProfID&quot; names(hor)[2] &lt;- &quot;HorID&quot; # scan the data summary(site) summary(hor) # 3 - select useful columns ==================================================== ## 3.1 - select columns -------------------------------------------------------- hor &lt;- select(hor, ProfID, HorID, top, bottom, ph=ph_h2o, k, soc, clay, bd, cec) # 4 - Quality check ============================================================ ## 4.1 - Check locations ------------------------------------------------------- # https://epsg.io/6204 site %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) %&gt;% # convert to spatial object mapview(zcol = &quot;ProfID&quot;, cex = 3, lwd = 0.1) # visualise in an interactive map # profile 2823 is wrongly located, so let&#39;s remove it site &lt;- filter(site, ProfID != 2823) ## 4.2 - Convert data into a Soil Profile Collection --------------------------- depths(hor) &lt;- ProfID ~ top + bottom hor@site$ProfID &lt;- as.numeric(hor@site$ProfID) site(hor) &lt;- left_join(site(hor), site) profiles &lt;- hor profiles ## 4.3 - plot first 20 profiles using pH as color ------------------------------ plotSPC(x = profiles[1:20], name = &quot;cec&quot;, color = &quot;cec&quot;, name.style = &quot;center-center&quot;) ## 4.4 - check data integrity -------------------------------------------------- # A valid profile is TRUE if all of the following criteria are false: # + depthLogic : boolean, errors related to depth logic # + sameDepth : boolean, errors related to same top/bottom depths # + missingDepth : boolean, NA in top / bottom depths # + overlapOrGap : boolean, gaps or overlap in adjacent horizons aqp::checkHzDepthLogic(profiles) # visualize some of these profiles by the pid subset(profiles, grepl(6566, ProfID, ignore.case = TRUE)) subset(profiles, grepl(6915, ProfID, ignore.case = TRUE)) subset(profiles, grepl(7726, ProfID, ignore.case = TRUE)) ## 4.5 - keep only valid profiles ---------------------------------------------- clean_prof &lt;- HzDepthLogicSubset(profiles) metadata(clean_prof)$removed.profiles # write_rds(clean_prof, &quot;01-Data/soilProfileCollection.rds&quot;) ## 4.6 convert soilProfileCollection to a table -------------------------------- dat &lt;- left_join(clean_prof@site, clean_prof@horizons) dat &lt;- select(dat, ProfID, HorID, x, y, date, top, bottom, ph:cec ) # 5 - Estimate BD using pedotransfer functions ================================= # create the function with all PTF estimateBD &lt;- function(SOC=NULL, method=NULL){ OM &lt;- SOC * 1.724 if(method==&quot;Saini1996&quot;){BD &lt;- 1.62 - 0.06 * OM} if(method==&quot;Drew1973&quot;){BD &lt;- 1 / (0.6268 + 0.0361 * OM)} if(method==&quot;Jeffrey1979&quot;){BD &lt;- 1.482 - 0.6786 * (log(OM))} if(method==&quot;Grigal1989&quot;){BD &lt;- 0.669 + 0.941 * exp(1)^(-0.06 * OM)} if(method==&quot;Adams1973&quot;){BD &lt;- 100 / (OM /0.244 + (100 - OM)/2.65)} if(method==&quot;Honeyset_Ratkowsky1989&quot;){BD &lt;- 1/(0.564 + 0.0556 * OM)} return(BD) } ## 5.1 - Select a pedotransfer function ---------------------------------------- # create a vector of BD values to test the best fitting pedotransfer function BD_test &lt;- tibble(SOC = clean_prof@horizons$soc, BD_test = clean_prof@horizons$bd) BD_test &lt;- na.omit(BD_test) ## 5.2 - Estimate BLD for a subset using the pedotransfer functions ------------ BD_test$Saini &lt;- estimateBD(BD_test$SOC, method=&quot;Saini1996&quot;) BD_test$Drew &lt;- estimateBD(BD_test$SOC, method=&quot;Drew1973&quot;) BD_test$Jeffrey &lt;- estimateBD(BD_test$SOC, method=&quot;Jeffrey1979&quot;) BD_test$Grigal &lt;- estimateBD(BD_test$SOC, method=&quot;Grigal1989&quot;) BD_test$Adams &lt;- estimateBD(BD_test$SOC, method=&quot;Adams1973&quot;) BD_test$Honeyset_Ratkowsky &lt;- estimateBD(BD_test$SOC, method=&quot;Honeyset_Ratkowsky1989&quot;) ## 5.3 Compare results --------------------------------------------------------- # Observed values: summary(BD_test$BD_test) # Predicted values: summary(BD_test$Saini) summary(BD_test$Drew) summary(BD_test$Jeffrey) summary(BD_test$Grigal) summary(BD_test$Adams) summary(BD_test$Honeyset_Ratkowsky) # Compare data distributions for observed and predicted BLD plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,5), lwd=2, main=&quot;Bulk Density Pedotransfer Functions&quot;) lines(density(BD_test$Saini),col=&quot;green&quot;, lwd=2) lines(density(BD_test$Drew),col=&quot;red&quot;, lwd=2) lines(density(BD_test$Jeffrey),col=&quot;cyan&quot;, lwd=2) lines(density(BD_test$Grigal),col=&quot;orange&quot;, lwd=2) lines(density(BD_test$Adams),col=&quot;magenta&quot;, lwd=2) lines(density(BD_test$Honeyset_Ratkowsky),col=&quot;blue&quot;, lwd=2) legend(&quot;topleft&quot;, legend = c(&quot;Original&quot;, &quot;Saini&quot;, &quot;Drew&quot;, &quot;Jeffrey&quot;, &quot;Grigal&quot;, &quot;Adams&quot;, &quot;Honeyset_Ratkowsky&quot;), fill=c(&quot;black&quot;, &quot;green&quot;, &quot;red&quot;, &quot;cyan&quot;, &quot;orange&quot;,&quot;magenta&quot;, &quot;blue&quot;)) # Plot the Selected function again plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,3.5), lwd=2, main=&quot;Bulk Density Selected Function&quot;) lines(density(BD_test$Honeyset_Ratkowsky),col=&quot;blue&quot;, lwd=2) legend(&quot;topleft&quot;,legend = c(&quot;Original&quot;, &quot;Honeyset_Ratkowsky&quot;), fill=c(&quot;black&quot;, &quot;blue&quot;)) ## 5.4 Estimate BD for the missing horizons ------------------------------------ dat$bd[is.na(dat$bd)] &lt;- estimateBD(dat[is.na(dat$bd),]$soc, method=&quot;Honeyset_Ratkowsky1989&quot;) # Explore the results summary(dat$bd) plot(density(BD_test$BD_test),type=&quot;l&quot;,col=&quot;black&quot;, ylim=c(0,3.5), lwd=2, main=&quot;Bulk Density Gap-Filling&quot;) lines(density(dat$bd, na.rm = TRUE), col=&quot;green&quot;, lwd=2) legend(&quot;topleft&quot;,legend = c(&quot;Original&quot;, &quot;Original+Estimated&quot;), fill=c(&quot;black&quot;, &quot;green&quot;)) ## 5.5 - Explore outliers ------------------------------------------------------ # Outliers should be carefully explored and compared with literature values. # Only if it is clear that outliers represent impossible or highly unlikely # values, they should be removed as errors. # # Carbon content higher than 15% is only typical for organic soil (histosols) # We will remove all atypically high SOC as outliers summary(dat$soc) na.omit(dat$ProfID[dat$soc &gt; 10]) dat &lt;- dat[dat$ProfID != 6915,] dat &lt;- dat[dat$ProfID != 7726,] # Explore bulk density data, identify outliers # remove layers with Bulk Density &lt; 1 g/cm^3 low_bd_profiles &lt;- na.omit(dat$ProfID[dat$bd&lt;1]) dat &lt;- dat[!(dat$ProfID %in% low_bd_profiles),] # Explore data, identify outliers x &lt;- pivot_longer(dat, cols = ph:cec, values_to = &quot;value&quot;, names_to = &quot;soil_property&quot;) x &lt;- na.omit(x) ggplot(x, aes(x = soil_property, y = value, fill = soil_property)) + geom_boxplot() + facet_wrap(~soil_property, scales = &quot;free&quot;) # 6 - Harmonize soil layers ==================================================== ## 6.1 - Set target soil properties and depths --------------------------------- names(dat) dat &lt;- select(dat, ProfID, HorID, x, y, top, bottom, ph, k, soc, clay, bd, cec) target &lt;- c(&quot;ph&quot;, &quot;k&quot;, &quot;soc&quot;, &quot;clay&quot;, &quot;bd&quot;, &quot;cec&quot;) depths &lt;- t(c(0,30)) ## 6.2 - Create standard layers ------------------------------------------------ d &lt;- unique(select(dat, ProfID, x, y)) for (i in seq_along(target)) { vlow &lt;- min(dat[,target[i]][[1]], na.rm = TRUE) vhigh &lt;- max(dat[,target[i]][[1]], na.rm = TRUE) o &lt;- dat[,c(&quot;ProfID&quot;, &quot;top&quot;, &quot;bottom&quot;,target[i])] %&gt;% na.omit() %&gt;% as.data.frame(stringsAsFactors = FALSE) x &lt;- ithir::ea_spline(obj = o, var.name = target[i], d = depths, vlow = vlow[[1]], vhigh = vhigh[[1]])$harmonised x[x==-9999] &lt;- NA x &lt;- x %&gt;% as_tibble() %&gt;% select(-`soil depth`) names(x) &lt;- c(&quot;ProfID&quot;,paste0(target[i],c(&quot;_0_30&quot;,&quot;_30_60&quot;,&quot;_60_100&quot;))) d &lt;- d %&gt;% left_join(x, by = &quot;ProfID&quot; ) } d # 7 - Plot and save results =================================================== x &lt;- pivot_longer(d, cols = ph_0_30:cec_0_30, values_to = &quot;value&quot;, names_sep = &quot;_&quot;, names_to = c(&quot;soil_property&quot;, &quot;top&quot;, &quot;bottom&quot;)) x &lt;- mutate(x, depth = paste(top, &quot;-&quot; , bottom)) x &lt;- na.omit(x) ggplot(x, aes(x = depth, y = value, fill = soil_property)) + geom_boxplot() + facet_wrap(~soil_property, scales = &quot;free&quot;) # remove BD and CF # d &lt;- select(d, ProfID:y, soc_0_30:ocs_60_100) # save data write_csv(d, &quot;02-Outputs/spline_soil_profile.csv&quot;) 9.2 Download environmental covariates #Empty environment and cache rm(list = ls()); gc() # Content of this script ======================================================= # The goal of this script is to organise to clip and dowload the covariates and # it includes the following steps: # # 0 -User-defined variables # 1 - Set working directory and load necessary packages # 2 - Import shapefile # 3 - Overview of covariates # 4 - Initialize GEE # 5 - Upload shapefile to GEE OR use uploaded UN borders # 6 - Clip and download the covariates #_______________________________________________________________________________ # 0 - User-defined variables =================================================== # Working directory #wd &lt;- &#39;C:/Users/luottoi/Documents/GitHub/Digital-Soil-Mapping&#39; wd &lt;- &#39;C:/GIT/Digital-Soil-Mapping&#39; # Output covariate folder #output_dir &lt;-&#39;&#39; output_dir &lt;-&#39;01-Data/covs/&#39; # Area of interest: either own shapefile or 3-digit ISO code to extract from # UN 2020 boundaries AOI &lt;- &#39;01-Data/AOI_Arg.shp&#39; # AOI &lt;- &#39;MKD&#39; # Resolution and projection res = 250 crs = &quot;EPSG:4326&quot; #_______________________________________________________________________________ # 1 - Set working directory and load necessary packages ======================= # Set working directory setwd(wd) #load libraries library(raster) library(terra) library(sf) library(rgee) # 2 - Import shapefile ========================================================= AOI &lt;- read_sf(AOI) # convert AOI to a box polygon AOI &lt;- st_as_sfc(st_bbox(AOI)) AOI &lt;- st_as_sf(AOI) # 3 - Overview of covariates =================================================== # CLIMATIC VARIABLES from CHELSA # VEGETATION INDICES, FPAR and LAND SURFACE TEMPERATURE from MODIS # LAND COVER LAYERS from Dynamic World 10m near-real-time (NRT) # TERRAINE attributes from OpenLandMap # for more information about the single covariates: open covariates.xslx in the # training material folder # 4 - Initialize GEE =========================================================== ee_Initialize() # 5 - Upload shapefile to GEE OR use uploaded UN borders ======================= ## 5.1 Convert shp to gee geometry --------------------------------------------- #region &lt;- sf_as_ee(AOI) #region = region$geometry() ## 5.2 Extract from UN 2020 map using ISO code --------------------------------- region &lt;-ee$FeatureCollection(&quot;projects/digital-soil-mapping-gsp-fao/assets/UN_BORDERS/BNDA_CTY&quot;)%&gt;% ee$FeatureCollection$filterMetadata(&#39;ISO3CD&#39;, &#39;equals&#39;, AOI) region = region$geometry() # AOI_shp &lt;-ee_as_sf(region) # AOI_shp &lt;- st_collection_extract(AOI_shp, &quot;POLYGON&quot;) # write_sf(AOI_shp, paste0(&#39;01-Data/&#39;,AOI,&#39;.shp&#39;)) # aoi &lt;- vect(AOI_shp) # 6 - Clip and download covariates ======================================== # Obtain list of climatic variables assetname&lt;- rbind(ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER&quot;), ee_manage_assetlist(path_asset = &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP&quot;)) assetname$num &lt;- 1:nrow(assetname) # Loop over the names of assets to clip and dowload the covariates for (i in unique(assetname$ID)){ #Extract filename filename &lt;- sub(&#39;.*\\\\/&#39;, &#39;&#39;, i) #Clip image to the extent of the AOI image &lt;- ee$Image(i) %&gt;% ee$Image$clip(region)%&gt;% ee$Image$toFloat() # Resample to target resolution image = image$resample(&#39;bilinear&#39;)$reproject( crs= crs, scale= res) #Export clipped covariate as raster raster &lt;- ee_as_raster( image = image, scale= res, region = region, via = &quot;drive&quot;, maxPixels = 1e+12 ) plot(raster) num &lt;- assetname[assetname$ID == i, &#39;num&#39;] writeRaster(raster, paste0(output_dir,filename, &#39;.tif&#39;), overwrite=T) print(paste(filename, &#39;exported successfully - Covariate&#39;,num, &#39;out of 68&#39;)) } 9.3 Modelling, validation and prediction using soil data with coordinates #_______________________________________________________________________________ # # Quantile Regression Forest # Soil Property Mapping # # GSP-Secretariat # Contact: Isabel.Luotto@fao.org # Marcos.Angelini@fao.org #_______________________________________________________________________________ #Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================================= # 0 - Set working directory, soil attribute, and packages # 1 - Merge soil data with environmental covariates # 2 - Covariate selection # 3 - Model calibration # 4 - Uncertainty assessment # 5 - Prediction # 6 - Export final maps #_______________________________________________________________________________ # 0 - Set working directory, soil attribute, and packages ====================== # Working directory setwd(&#39;C:/GIT/Digital-Soil-Mapping&#39;) # Load Area of interest (shp) AOI &lt;- &#39;01-Data/AOI_Arg.shp&#39; # Terget soil attribute soilatt &lt;- &#39;k&#39; # Function for Uncertainty Assessment load(file = &quot;03-Scripts/eval.RData&quot;) #load packages library(tidyverse) library(data.table) library(caret) library(quantregForest) library(terra) library(sf) library(doParallel) # 1 - Merge soil data with environmental covariates ============================ ## 1.1 - Load covariates ------------------------------------------------------- # files &lt;- list.files(path= &#39;01-Data/covs/&#39;, pattern = &#39;.tif$&#39;, full.names = T) # covs &lt;- rast(files) # ncovs &lt;- str_remove(files, &quot;01-Data/covs/&quot;) # ncovs &lt;- str_remove(ncovs, &quot;.tif&quot;) # ncovs &lt;- str_replace(ncovs, &quot;-&quot;, &quot;_&quot;) # names(covs) &lt;- ncovs covs &lt;- rast(&quot;01-Data/covs/covs.tif&quot;) ncovs &lt;- names(covs) ## 1.2 - Load the soil data (Script 2) ----------------------------------------- dat &lt;- read_csv(&quot;01-Data/data_with_coord.csv&quot;) # Convert soil data into a spatial object (check https://epsg.io/6204) dat &lt;- vect(dat, geom=c(&quot;x&quot;, &quot;y&quot;), crs = crs(covs)) # Reproject point coordinates to match coordinate system of covariates dat &lt;- terra::project(dat, covs) names(dat) ## 1.3 - Extract values from covariates to the soil points --------------------- pv &lt;- terra::extract(x = covs, y = dat, xy=F) dat &lt;- cbind(dat,pv) dat &lt;- as.data.frame(dat) summary(dat) ## 1.4 - Target soil attribute + covariates ------------------------------------ d &lt;- select(dat, soilatt, names(covs)) d &lt;- na.omit(d) # 2 - Covariate selection with RFE ============================================= ## 2.1 - Setting parameters ---------------------------------------------------- # Repeatedcv = 3-times repeated 10-fold cross-validation fitControl &lt;- rfeControl(functions = rfFuncs, method = &quot;repeatedcv&quot;, number = 10, ## 10 -fold CV repeats = 3, ## repeated 3 times verbose = TRUE, saveDetails = TRUE, returnResamp = &quot;all&quot;) # Set the regression function fm = as.formula(paste(soilatt,&quot; ~&quot;, paste0(ncovs, collapse = &quot;+&quot;))) # Calibrate the model using multiple cores cl &lt;- makeCluster(detectCores()-1) registerDoParallel(cl) ## 2.2 - Calibrate a RFE model to select covariates ---------------------------- covsel &lt;- rfe(fm, data = d, sizes = seq(from=10, to=length(ncovs)-1, by = 5), rfeControl = fitControl, verbose = TRUE, keep.inbag = T) stopCluster(cl) ## 2.3 - Plot selection of covariates ------------------------------------------ trellis.par.set(caretTheme()) plot(covsel, type = c(&quot;g&quot;, &quot;o&quot;)) # Extract selection of covariates and subset covs opt_covs &lt;- predictors(covsel) # 3 - QRF Model calibration ==================================================== ## 3.1 - Update formula with the selected covariates --------------------------- fm &lt;- as.formula(paste(soilatt,&quot; ~&quot;, paste0(opt_covs, collapse = &quot;+&quot;))) # parallel processing cl &lt;- makeCluster(detectCores()-1) registerDoParallel(cl) ## 3.2 - Set training parameters ----------------------------------------------- fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, ## 10 -fold CV repeats = 3, ## repeated 3 times savePredictions = TRUE) # Tune mtry hyperparameters mtry &lt;- round(length(opt_covs)/3) tuneGrid &lt;- expand.grid(mtry = c(mtry-5, mtry, mtry+5)) ## 3.3 - Calibrate the QRF model ----------------------------------------------- model &lt;- caret::train(fm, data = d, method = &quot;qrf&quot;, trControl = fitControl, verbose = TRUE, tuneGrid = tuneGrid, keep.inbag = T, importance = TRUE) stopCluster(cl) gc() ## 3.4 - Extract predictor importance as relative values (%) x &lt;- randomForest::importance(model$finalModel) model$importance &lt;- x ## 3.5 - Print and save model -------------------------------------------------- print(model) saveRDS(model, file = paste0(&quot;02-Outputs/models/model_&quot;,soilatt,&quot;.rds&quot;)) # 4 - Uncertainty assessment =================================================== # extract observed and predicted values o &lt;- model$pred$obs p &lt;- model$pred$pred df &lt;- data.frame(o,p) ## 4.1 - Plot and save scatterplot --------------------------------------------- (g1 &lt;- ggplot(df, aes(x = o, y = p)) + geom_point(alpha = 0.1) + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;)+ ylim(c(min(o), max(o))) + theme(aspect.ratio=1)+ labs(title = soilatt) + xlab(&quot;Observed&quot;) + ylab(&quot;Predicted&quot;)) ggsave(g1, filename = paste0(&quot;02-Outputs/residuals_&quot;,soilatt,&quot;.png&quot;), scale = 1, units = &quot;cm&quot;, width = 12, height = 12) ## 4.2 - Print accuracy coeficients -------------------------------------------- # https://github.com/AlexandreWadoux/MapQualityEvaluation eval(o,p) ## 4.3 - Plot Covariate importance --------------------------------------------- (g2 &lt;- varImpPlot(model$finalModel, main = soilatt, type = 1)) png(filename = paste0(&quot;02-Outputs/importance_&quot;,soilatt,&quot;.png&quot;), width = 15, height = 15, units = &quot;cm&quot;, res = 600) g2 dev.off() # 5 - Prediction =============================================================== # Generation of maps (prediction of soil attributes) ## 5.1 - Produce tiles --------------------------------------------------------- r &lt;-covs[[1]] t &lt;- rast(nrows = 5, ncols = 5, extent = ext(r), crs = crs(r)) tile &lt;- makeTiles(r, t,overwrite=TRUE,filename=&quot;02-Outputs/tiles/tiles.tif&quot;) ## 5.2 - Predict soil attributes per tiles ------------------------------------- # loop to predict on each tile for (j in seq_along(tile)) { gc() t &lt;- rast(tile[j]) covst &lt;- crop(covs, t) # plot(r)# pred_mean &lt;- terra::predict(covst, model = model$finalModel, na.rm=TRUE, cpkgs=&quot;quantregForest&quot;, what=mean) pred_sd &lt;- terra::predict(covst, model = model$finalModel, na.rm=TRUE, cpkgs=&quot;quantregForest&quot;, what=sd) # ###### Raster package solution (in case terra results in many NA pixels) # library(raster) # covst &lt;- stack(covst) # class(final_mod$finalModel) &lt;-&quot;quantregForest&quot; # # Estimate model uncertainty # pred_sd &lt;- predict(covst,model=final_mod$finalModel,type=sd) # # OCSKGMlog prediction based in all available data # pred_mean &lt;- predict(covst,model=final_mod) # # # ################################## writeRaster(pred_mean, filename = paste0(&quot;02-Outputs/tiles/soilatt_tiles/&quot;, soilatt,&quot;_tile_&quot;, j, &quot;.tif&quot;), overwrite = TRUE) writeRaster(pred_sd, filename = paste0(&quot;02-Outputs/tiles/soilatt_tiles/&quot;, soilatt,&quot;_tileSD_&quot;, j, &quot;.tif&quot;), overwrite = TRUE) rm(pred_mean) rm(pred_sd) print(paste(&quot;tile&quot;,tile[j])) } ## 5.3 - Merge tiles both prediction and st.Dev -------------------------------- f_mean &lt;- list.files(path = &quot;02-Outputs/tiles/soilatt_tiles/&quot;, pattern = paste0(soilatt,&quot;_tile_&quot;), full.names = TRUE) f_sd &lt;- list.files(path = &quot;02-Outputs/tiles/soilatt_tiles/&quot;, pattern = paste0(soilatt,&quot;_tileSD_&quot;), full.names = TRUE) r_mean_l &lt;- list() r_sd_l &lt;- list() for (g in 1:length(f_mean)){ r &lt;- rast(f_mean[g]) r_mean_l[g] &lt;-r rm(r) } for (g in 1:length(f_sd)){ r &lt;- rast(f_sd[g]) r_sd_l[g] &lt;-r rm(r) } r_mean &lt;-sprc(r_mean_l) r_sd &lt;-sprc(r_sd_l) pred_mean &lt;- mosaic(r_mean) pred_sd &lt;- mosaic(r_sd) AOI &lt;- vect(AOI) pred_mean &lt;- mask(pred_mean,AOI) pred_sd &lt;- mask(pred_sd,AOI) plot(pred_mean) plot(pred_sd) # 6 - Export final maps ======================================================== ## 6.1 - Mask croplands -------------------------------------------------------- msk &lt;- rast(&quot;01-Data/mask_arg.tif&quot;) plot(msk) pred &lt;- mask(pred_mean, msk) plot(pred) ## 6.2 - Save results ---------------------------------------------------------- writeRaster(pred_mean, paste0(&quot;02-Outputs/maps/&quot;,soilatt,&quot;_QRF.tif&quot;), overwrite=TRUE) writeRaster(pred_sd, paste0(&quot;02-Outputs/maps/&quot;,soilatt,&quot;_QRF_SD.tif&quot;), overwrite=TRUE) 9.4 Annex A: R scripts for extra functions Script to estimate Organic Carbon Stock "],["reporting-results.html", "Chapter 10 Reporting results", " Chapter 10 Reporting results "],["way-forward.html", "Chapter 11 Way forward", " Chapter 11 Way forward "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
