# Arranging soil data in **R**

In this chapter, the datasets used in this technical manual are presented. Then, step-by-step instructions are given on how to carry out necessary steps in **RStudio** in order to be able to work with your soil data and generate maps. Instructions are given on how to:

1. Generate user-defined variables, 
2. Set the working directory and load necessary packages, 
3. Import national data to **R Studio**
4. Handle data (select, filter useful columns, etc.)

Thus, the instructions also serve as a continuation of the basic introduction to the functioning of **R** and *RStudio* given in [Chapter 3](https://fao-gsp.github.io/GSNmap-TM/setting-up-the-software-environment.html#use-of-r-rstudio-and-r-packages). Still, in case for further information there is a vast amount of websites that offer help and or information on **R** and *RStudio*.

## Study area and training material

The study area is located in the southeast of the Pampas Region, in Argentina, from the foothills of the Ventania and Tandilia hill systems, until the southern coasts of the Buenos Aires Province. 
To illustrate the different processes of this Technical Manual, we use three datasets from this region:

* Georeferenced topsoil data
* Non-georeferenced topsoil data
* Soil profile data

### Georeferenced topsoil data

These data were collected in 2011 by the National Institute of Agriculture Technology and Faculty of Agricultural Science of the National University of Mar del Plata  (Unidad Integrada INTA-FCA) to map the status of soil nutrients in the Argentinian Pampas [@sainz2019]. The dataset is a subset with 118 locations of topsoil samples (0-20 cm) that contains measurements of Organic Matter (om), P Bray (p_bray), pH (ph), and K (k) (Table \@ref(tab:table2)) and is shown in the following map as points. This dataset is used in [Chapter 8](https://fao-gsp.github.io/GSNmap-TM/step-3-mapping-continuous-soil-properties.html) for mapping K. 

```{r table2, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
kable(booktabs = T, data[1:10,], col.names = gsub("[.]", " ", names(data)), caption = 'Dataset with coordinates for mapping nutrients.', format = 'html', digits = c(0,6,6,2,2,2,1)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only the ten first rows are shown.", general_title = "")
```

```{r exploratory1, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(mapview)

mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
s <- st_as_sf(data, coords = c("x", "y"), crs = 4326)
mapview(s, zcol = "p_bray", cex = 2.5, lwd = 0)
```

### Non-georeferenced topsoil data 

The second dataset consist of topsoil samples (0-20cm) of P Bray (p_bray) without geographical coordinates collected during 2005 by @sainz2011 to map the status of soil nutrients in the Argentinian Pampas. Each sample is linked to an administrative unit (Partido) of the Buenos Aires Province (Table \@ref(tab:table3)). This dataset is used in [Annex III: Mapping without point coordinates](https://fao-gsp.github.io/GSNmap-TM/annex-iii-mapping-without-point-coordinates.html)


```{r table3, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("Digital-Soil-Mapping/01-Data/Buenos_Aires_sur_P_Bray.csv")
kable(booktabs = T, data[c(1:5,4001:4005),], col.names = gsub("[.]", " ", names(data)), caption = 'Dataset with coordinates for mapping nutrients.', format = 'html', digits = c(0,6,6,2,2,2,1)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only ten rows are shown.", general_title = "")
```

The following map shows the districts with colours according to the sample size (n).

```{r exploratory data 2, eval=TRUE, message=FALSE, warning=FALSE, hide = T}
library(tidyverse)
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/Buenos_Aires_sur_P_Bray.csv")
district <- st_read("Digital-Soil-Mapping/01-Data/district.shp", quiet = T)
mapview(district, zcol = "n", lwd = 0)
```

### Soil profile data

Finally, the third dataset belongs to the Soil Information System of Argentina ([SISINTA](http://sisinta.inta.gob.ar/), @Olmedo2017) which contains soil profiles collected from the sixties to recently years for soil survey purposes. The data can be fetched using the package [SISINTAR](https://github.com/INTA-Suelos/SISINTAR#readme). Table \@ref(tab:table4) shows a subset of the data, and the map presents the distribution of soil profiles for the study area. This dataset is used in this chapter to illustrate the preprocessing steps required for data that come from soil profiles.

```{r table4, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("Digital-Soil-Mapping/01-Data/soil_profile_data.csv")
kable(booktabs = T, data[c(1:10),1:10], col.names = gsub("[.]", " ", names(data)[1:10]), caption = 'Soil profile dataset.', format = 'html', digits = c(0,0,6,6,0,0,0,1,1,2)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only ten rows are shown.", general_title = "")
```

```{r exploratory data 3, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/soil_profile_data.csv")
s <- data %>% filter(top==0)
s <- st_as_sf(s, coords = c("x", "y"), crs = 4326)
mapview(s, zcol = "k", cex = 2.5, lwd = 0)
```

## Format requirements of soil data {#preproc}

Soil data consist of measurement at a specific geographical location, time and soil depth. Therefore, it is necessary to arrange the data following the format shown in Table \@ref(tab:data1).

```{r data1, echo = FALSE, message=F, warning=F}
options(knitr.table.format = "HTML")
library(dplyr)
library(kableExtra)
dt <- read.csv("tables/Table_5.1.csv", sep = ",")
kable(booktabs = T, dt, col.names = gsub("[.]", " ", names(dt)), caption = 'Example format of a database.', format = 'html') %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Profile ID = unique profile identifier, Horizon ID = unique layer identifier, Lat = latitude in decimal degrees, Long = longitude in decimal degrees, Year = sampling year, Top = upper limit of the layer in cm, Bottom = lower limit of the layer in cm, cec = Cation Exchange Capacity (cmol_c/kg), ph = pH in water, clay = Clay (g/100g soil), silt = Silt (g/100g soil), sand = Sand ((g/100g soil), soc = Soil Organic Carbon (g/100g soil), bd = Bulk Density (g/cm3).", threeparttable = TRUE)
  #add_header_above(c("Approach for developing national information on soil erosion" = 2), bold = T) %>% 
  #kableExtra::group_rows(group_label = "Approach for developing national information on soil erosion", start_row = 1, end_row = 3) %>%
  #kableExtra::group_rows(group_label = "Input data preparation (see Table 3.1)", start_row = 4, end_row = 10) %>% 
  #kableExtra::group_rows(group_label = "Expected outputs", start_row = 11, end_row = 19)
#knitr::kable()

```

## Pre-processing steps
Soil data is often arranged in a different way which requires specific pre-processing steps to reach the format. On the way towards a formatted database, common issues such as, arranging the data format, fixing soil horizon depth consistency, detecting unusual soil property measurements, can be solved. Here, common issues and examples are given on how to carry out some basic data handling steps in *RStudio*.

### Set the scene (set working directory, packages, load data)

So, let's open *RStudio*. Whenever starting to work on a project or task, it is necessary to set the *working directory* (WD). The WD is the folder path that is used by **R** to save the output, for instance a plot or a table that was generated while working in **R**. Thus, the WD is central since it dictates where the files and calculations can be found afterwards. As it is so important, there are multiple ways of setting the WD. 
One option is to right click on 'Session' menu > 'Set working directory ...' and select either 'To Source File Location' (then the WD corresponds to the file path where the Script is saved to) or 'Choose Directory...'. Then, the user can browse to the folder that should be the WD. 

In this manual we propose an alternative way that allows for more customization and flexibility since sometimes multiple WDs are needed to for instance save the final map in a different folder than the covariates. Since the file paths differ depending on where you stored the file on your computer, it is crucial to identify the correct file path. This can be done by accessing the *file explorer*. There you can browse to your training material folder and then right-click on the bar highlighted in red in the Figure \@ref(fig:explorer).

```{r explorer, echo = FALSE, fig.cap = "Get file path from file explorer."}
knitr::include_graphics("images/file-explorer.png")

```

The file path will appear with the following format: `C:\Users\GSNmap-TM\Digital-Soil-Mapping`. In order to enable **R** to read this as file path, it is necessary to replace the `\` by `/`. The resulting file path should look similar to this one: `C:/Users/GSNmap-TM/Digital-Soil-Mapping`. Once this is done, we can assign the file path that represents the WD file path to an **R** object. This is done by defining a character value (in this case the file path) on the right side of the arrow (`<-`) and name the **R** object on the left side (wd) (see code). Once this is done we use the function `setwd()` to set the WD to the file path that is specified in the object `wd`.

```{r, echo = T, eval = F}
# 0 - User-defined variables ===================================================
wd <- 'C:/Users/hp/Documents/GitHub/Digital-Soil-Mapping'
#wd <- "C:/GIT/Digital-Soil-Mapping"

# 1 - Set working directory and load necessary packages ========================
setwd(wd) # change the path accordingly
```

Next to in-built base R functions, there is a vast amount of so-called packages that extend the functionalities of **R** and allow the use of **R** for a broad range of purposes. For data handling and management, the `tidyverse` package and its dependencies offer a great help. To load packages into the *RStudio* session, the `library` function is used. However, if the package is not installed, it is necessary to use the `install.packages` function first.

```{r, echo = T, eval = T}

#install.packages(tidyverse)
library(readxl)
library(tidyverse)
library(dplyr)

# load in data
data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
head(data)

```

### Basic data handling operations
In this section, basic operations with data in **R** are covered. It is explained how to select columns, filter observations/rows by certain values, remove missing values (NAs), how to rename columns and finally how to check the structure and classes of the whole dataframe or specific columns.
The loaded dataset may comprise columns that are not of interest for the specific task you are working on. Therefore, it is recommendable to select the columns that are relevant to keep your working environment in *RStudio* clean. In the following example we specify the dataframe we want to select columns from and then link the dataset to the `select` function of the tidyverse to select only the coordinates and the ID column. Then, we store the selected columns in a new object called locations.

```{r, echo = T, eval = T}
# Select columns
locations <- data %>% dplyr::select(LabID, x, y)
head(locations)
```

Another important operation is to filter by certain row values. For that, we can use the `filter` function of the tidyverse. Here, we want to filter to have only samples that are located below -38 degrees latitude (y). To do this, we can follow the same syntax as in the example above

```{r, echo = T, eval = T}
# Filter
south_locations <- data %>% filter(y <= -38)
head(south_locations)
```

One key operation is to rename column names. This is highly recommended before you start running the scripts explained in the following chapters. **R** is case-sensitive. This means that it matters whether you write `ph`, `PH`, or `pH`. Therefore, it is of utmost importance to be consistent and aware of any typos. To rename columns, it may be of interest to know how the columns are named. This can be checked by the `names()` function.
Let's say we want to rename the "LabID" column and the "p_bray" column to "ID" and "p" in the south_locations dataframe with base R. The way to do this, is to assign the names of the columns of the dataframe to a vector (here called `names`) and select the name of a specific column in the vector by using squared brackets `[]` and a number that indicates the position of the column in the vector. Then, you can replace it with another name.
Alternatively, one can follow the already known syntax of the tidyverse. Here, we rename three columns of the locations dataset using the `rename` function. Note that the new name is specified on the left side of the equal sign and the name to be replaced on the right side. 

```{r, echo = T, eval = T}
#check names
names(south_locations)
names <- names(south_locations)
# renaming - base R option
names[1] <- "ID"
names[6] <- "p"
names(south_locations)

# renaming - tidyverse option
names(locations)
locations1 <- locations %>% rename(
  ID = LabID,
  long = x,
  lat = y
)
names(locations1)


```

Another important operation when working with soil data is to remove NA values. NA values are empty cells that for instance do not contain coordinates. In the following case, we are going to check whether the locations dataset has NA values in the x column. To this end, we can use the `is.na` function. This function returns logical (TRUE or FALSE) values. Based on these values, any NA observations that have NA values in the x column can be removed from the dataset. For that we use again squared brackets `[]` and the logical operator `!` that equals `is not` to select all observations of locations that don't have NA values in the x column.

```{r, echo = T, eval = T}
# identify NAs
is.na(locations$x)

locations <- locations[!is.na(locations$x),]
head(locations)
```

Finally, another common source of error arises from wrongly classified columns. For instance, if the column containing the pH measurements is not numeric but is classified as character (means text). To check which classes were assigned to each column after reading in the data, it is recommendable to use `str()` and `summary()` to get an overview of the dataset.
In case the variables are not assigned to the correct class one can use the `as.numeric` or `as.character` functions to convert the respective column. However, in the present case all variables are assigned correctly. If it is not the case with your soil data, there might be issues with some observations, e.g. text values in certain observations.

```{r, echo = T, eval = T}
# check for class
str(data)
summary(data)
# change class of columns

# to numeric
#data$ph <- as.numeric(data$ph)

# to character
#data$LabID <- as.character(data$LabID)

```

For further guidance and more in-depth techniques to administer and handle soil data in **R**, it is recommended to check the GitHub repository on soil database management of the GSP: [FAO-GSP Soil DB](https://github.com/FAO-GSP/SoilDB). There, not only training data but also extensive example codes are available.
For now, we continue working with the example dataset and assume that the dataset you are using complies with the format specified at the beginning.