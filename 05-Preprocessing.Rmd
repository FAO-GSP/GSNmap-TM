# Preprocessing
## Format requirements and pre-processing of soil data {#preproc}

Soil data consist of measurement at a specific geographical location, time and soil depth. Therefore, it is necessary to arrange the data following the format shown in Table \@ref(tab:data1).

```{r data1, echo = FALSE, message=F, warning=F}
options(knitr.table.format = "HTML")
library(dplyr)
library(kableExtra)
dt <- read.csv("tables/Table_5.1.csv", sep = ",")
kable(booktabs = T, dt, col.names = gsub("[.]", " ", names(dt)), caption = 'Example format of a database.', format = 'html') %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Profile ID = unique profile identifier, Horizon ID = unique layer identifier, Lat = latitude in decimal degrees, Long = longitude in decimal degrees, Year = sampling year, Top = upper limit of the layer in cm, Bottom = lower limit of the layer in cm, cec = Cation Exchange Capacity (cmol_c/kg), ph = pH in water, clay = Clay (g/100g soil), silt = Silt (g/100g soil), sand = Sand ((g/100g soil), soc = Soil Organic Carbon (g/100g soil), bd = Bulk Density (g/cm3).", threeparttable = TRUE)
  #add_header_above(c("Approach for developing national information on soil erosion" = 2), bold = T) %>% 
  #kableExtra::group_rows(group_label = "Approach for developing national information on soil erosion", start_row = 1, end_row = 3) %>%
  #kableExtra::group_rows(group_label = "Input data preparation (see Table 3.1)", start_row = 4, end_row = 10) %>% 
  #kableExtra::group_rows(group_label = "Expected outputs", start_row = 11, end_row = 19)
#knitr::kable()

```

In this chapter, step-by-step instructions are given on how to carry out all these steps in **RStudio**. Instructions are given on how to:

1. Generate user-defined variables, 
2. Set the working directory and load necessary packages, 
3. Import national data to **R Studio**
4. Select useful columns
5. Perform a quality check of the data
6. Estimate bulk density using PTF
<!-- * estimate organic carbon stocks (OCS) -->
7. Harmonize soil layers (using splines)
8. Plot and save the formatted soil data

Thus, the instructions also serve as a very basic introduction to the functioning of **R** and *RStudio*. Still, in case for further information there is a vast amount of websites that offer help and or information on **R** and *RStudio*.

Soil data usually require a pre-processing step to solve common issues such as, arranging the data format, fixing soil horizon depth consistency, detecting unusual soil property measurements, among others issues. Here, common issues and examples are given on how to carry out some basic data handling steps in *RStudio*. 

So, let's open *RStudio*. Whenever starting to work on a project or task, it is necessary to set the *working directory* (WD). The WD is the folder path that is used by **R** to save the output, for instance a plot or a table that was generated while working in **R**. Thus, the WD is central since it dictates where the files and calculations can be found afterwards. As it is so important, there are multiple ways of setting the WD. 
One option is to right click on 'Session' menu > 'Set working directory ...' and select either 'To Source File Location' (then the WD corresponds to the file path where the Script is saved to) or 'Choose Directory...'. Then, the user can browse to the folder that should be the WD. 

In this manual we propose an alternative way that allows for more customization and flexibility since sometimes multiple WDs are needed to for instance save the final map in a different folder than the covariates. Since the file paths differ depending on where you stored the file on your computer, it is crucial to identify the correct file path. This can be done by accessing the *file explorer*. There you can browse to your training material folder and then right-click on the bar highlighted in red in the Figure \@ref(fig:explorer).

```{r explorer, echo = FALSE, fig.cap = "Get file path from file explorer."}
knitr::include_graphics("images/file-explorer.png")

```

The file path will appear with the following format: `C:\Users\GSNmap-TM\Digital-Soil-Mapping`. In order to enable **R** to read this as file path, it is necessary to replace the `\` by `/`. The resulting file path should look similar to this one: `C:/Users/GSNmap-TM/Digital-Soil-Mapping`. Once this is done, we can assign the file path that represents the WD file path to an **R** object. This is done by defining a character value (in this case the file path) on the right side of the arrow (`<-`) and name the **R** object on the left side (wd) (see code). Once this is done we use the function `setwd()` to set the WD to the file path that is specified in the object `wd`.

```{r, echo = T, eval = F}
# 0 - User-defined variables ===================================================
wd <- 'C:/Users/hp/Documents/GitHub/Digital-Soil-Mapping'
#wd <- "C:/GIT/Digital-Soil-Mapping"

# 1 - Set working directory and load necessary packages ========================
setwd(wd) # change the path accordingly
```

Next to in-built base R functions, there is a vast amount of so-called packages that extend the functionalities of **R** and allow the use of **R** for a broad range of purposes. For data handling and management, the `tidyverse` package and its dependencies offer a great help. To load packages into the *RStudio* session, the `library` function is used. However, if the package is not installed, it is necessary to use the `install.packages` function first.

```{r, echo = T, eval = T}

#install.packages(tidyverse)
library(tidyverse)
library(readxl)

# load in data
data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
head(data)

```

The loaded dataset may comprise columns that are not of interest for the specific task you are working on. Therefore, it is recommendable to select the columns that are relevant to keep your working environment in *RStudio* clean. In the following example we specify the dataframe we want to select columns from and then link the dataset to the `select` function of the tidyverse to select only the coordinates and the ID column. Then, we store the selected columns in a new object called locations.

```{r, echo = T, eval = T}
# Select columns
locations <- data %>% select(LabID, x, y)
head(locations)
```

Another important operation is to filter by certain row values. For that, we can use the `filter` function of the tidyverse. Here, we want to filter to have only samples that are located below -38 degrees latitude (y). To do this, we can follow the same syntax as in the example above

```{r, echo = T, eval = T}
# Filter
south_locations <- data %>% filter(y <= -38)
head(south_locations)
```

One key operation is to rename column names. This is highly recommended before you start running the scripts explained in the following chapters. **R** is case-sensitive. This means that it matters whether you write `ph`, `PH`, or `pH`. Therefore, it is of utmost importance to be consistent and aware of any typos. To rename columns, it may be of interest to know how the columns are named. This can be checked by the `names()` function.
Let's say we want to rename the "LabID" column and the "p_bray" column to "ID" and "p" in the south_locations dataframe with base R. The way to do this, is to assign the names of the columns of the dataframe to a vector (here called `names`) and select the name of a specific column in the vector by using squared brackets `[]` and a number that indicates the position of the column in the vector. Then, you can replace it with another name.
Alternatively, one can follow the already known syntax of the tidyverse. Here, we rename three columns of the locations dataset using the `rename` function. Note that the new name is specified on the left side of the equal sign and the name to be replaced on the right side. 

```{r, echo = T, eval = T}
#check names
names(south_locations)
names <- names(south_locations)
# renaming - base R option
names[1] <- "ID"
names[6] <- "p"
names(south_locations)

# renaming - tidyverse option
names(locations)
locations1 <- locations %>% rename(
  ID = LabID,
  long = x,
  lat = y
)
names(locations1)


```

Another important operation when working with soil data is to remove NA values. NA values are empty cells that for instance do not contain coordinates. In the following case, we are going to check whether the locations dataset has NA values in the x column. To this end, we can use the `is.na` function. This function returns logical (TRUE or FALSE) values. Based on these values, any NA observations that have NA values in the x column can be removed from the dataset. For that we use again squared brackets `[]` and the logical operator `!` that equals `is not` to select all observations of locations that don't have NA values in the x column.

```{r, echo = T, eval = T}
# identify NAs
is.na(locations$x)

locations <- locations[!is.na(locations$x),]
head(locations)
```

Finally, another common source of error arises from wrongly classified columns. For instance, if the column containing the pH measurements is not numeric but is classified as character (means text). To check which classes were assigned to each column after reading in the data, it is recommendable to use `str()` and `summary()` to get an overview of the dataset.
In case the variables are not assigned to the correct class one can use the `as.numeric` or `as.character` functions to convert the respective column. However, in the present case all variables are assigned correctly. If it is not the case with your soil data, there might be issues with some observations, e.g. text values in certain observations.

```{r, echo = T, eval = T}
# check for class
str(data)
summary(data)
# change class of columns

# to numeric
#data$ph <- as.numeric(data$ph)

# to character
#data$LabID <- as.character(data$LabID)

```
For further guidance and more in-depth techniques to administer and handle soil data in **R**, it is recommended to check the GitHub repository on soil database management of the GSP: [FAO-GSP Soil DB](https://github.com/FAO-GSP/SoilDB). There, not only training data but also extensive example codes are available.
For now, we continue working with the example dataset and assume that the dataset you are using complies with the format specified at the beginning.

## Study area and training material

The study area is located in the southeast of the Pampas Region, in Argentina, from the foothills of the Ventania and Tandilia hill systems, until the southern coasts of the Buenos Aires Province. 
To illustrate the different processes of this Technical Manual, we use three datasets from this region:

* Gereferenced topsoil data
* Non-georreferenced topsoil data
* Soil profile data

### Georeferenced topsoil data

These data were collected in 2011 by the National Institute of Agriculture Technology and Faculty of Agricultural Science of the National University of Mar del Plata  (Unidad Integrada INTA-FCA) to map the status of soil nutrients in the Argentinian Pampas [@sainz2019]. The dataset is a subset with 118 locations of topsoil samples (0-20 cm) that contains measurements of Organic Matter (om), P Bray (p_bray), pH (ph), and K (k) (Table \@ref(tab:table2)) and is shown in the following map as points. This dataset is used in [Chapter 7](https://fao-gsp.github.io/GSNmap-TM/step-3-mapping-continuous-soil-properties.html#step-3-mapping-continuous-soil-properties) for mapping K. 

```{r table2, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
kable(booktabs = T, data[1:10,], col.names = gsub("[.]", " ", names(data)), caption = 'Dataset with coordinates for mapping nutrients.', format = 'html', digits = c(0,6,6,2,2,2,1)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only the ten first rows are shown.", general_title = "")
```

```{r exploratory1, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(mapview)

mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/data_with_coord.csv")
s <- st_as_sf(data, coords = c("x", "y"), crs = 4326)
mapview(s, zcol = "p_bray", cex = 2.5, lwd = 0)
```

### Non-georeferenced topsoil data 

The second dataset consist of topsoil samples (0-20cm) of P Bray (p_bray) without geographical coordinates collected during 2005 by @sainz2011 to map the status of soil nutrients in the Argentinian Pampas. Each sample is linked to an administrative unit (Partido) of the Buenos Aires Province (Table \@ref(tab:table3)). This dataset is used in [Annex III: Mapping without point coordinates](https://fao-gsp.github.io/GSNmap-TM/annex-iii-mapping-without-point-coordinates.html)


```{r table3, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("Digital-Soil-Mapping/01-Data/Buenos_Aires_sur_P_Bray.csv")
kable(booktabs = T, data[c(1:5,4001:4005),], col.names = gsub("[.]", " ", names(data)), caption = 'Dataset with coordinates for mapping nutrients.', format = 'html', digits = c(0,6,6,2,2,2,1)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only ten rows are shown.", general_title = "")
```

The following map shows the districts with colours according to the sample size (n).

```{r exploratory data 2, eval=TRUE, message=FALSE, warning=FALSE, hide = T}
library(tidyverse)
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/Buenos_Aires_sur_P_Bray.csv")
district <- st_read("Digital-Soil-Mapping/01-Data/district.shp", quiet = T)
mapview(district, zcol = "n", lwd = 0)
```

### Soil profile data

Finally, the third dataset belongs to the Soil Information System of Argentina ([SISINTA](http://sisinta.inta.gob.ar/), @Olmedo2017) which contains soil profiles collected from the sixties to recently years for soil survey purposes. The data can be fetched using the package [SISINTAR](https://github.com/INTA-Suelos/SISINTAR#readme). Table \@ref(tab:table4) shows a subset of the data, and the map presents the distribution of soil profiles for the study area. This dataset is used in this chapter to illustrate the preprocessing steps required for data that come from soil profiles.

```{r table4, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("Digital-Soil-Mapping/01-Data/soil_profile_data.csv")
kable(booktabs = T, data[c(1:10),1:10], col.names = gsub("[.]", " ", names(data)[1:10]), caption = 'Soil profile dataset.', format = 'html', digits = c(0,0,6,6,0,0,0,1,1,2)) %>%
kable_classic(full_width = F) %>%
  #kable_styling(latex_options = 'striped', font_size = 10) %>%
footnote(general = "Only ten rows are shown.", general_title = "")
```

```{r exploratory data 3, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)
data <- read_csv("Digital-Soil-Mapping/01-Data/soil_profile_data.csv")
s <- data %>% filter(top==0)
s <- st_as_sf(s, coords = c("x", "y"), crs = 4326)
mapview(s, zcol = "k", cex = 2.5, lwd = 0)
```

## Preparatory steps
As specified above in the Section on [pre-processing](preproc) in the following lines of code, the working directory is set and the necessary R packages are loaded.
Note that there are many ways to install packages besides the most common way using the function `install.packages()`. For instance, to install the `terra` package, one has to write `install.packages("terra")`. This installs the package from CRAN. However, there are a few exceptions where development versions of R packages are required. In these instances additional packages such as `devtools` or `remotes` are needed (see example in code below). These packages are then able to install packages from for instance GitHub repositories.

```{r, eval = F}
# 0 - User-defined variables ===================================================
wd <- 'C:/Users/hp/Documents/GitHub/Digital-Soil-Mapping'
#wd <- "C:/GIT/Digital-Soil-Mapping"

# 1 - Set working directory and load necessary packages ========================
setwd(wd) # change the path accordingly

library(tidyverse) # for data management and reshaping
library(readxl) # for importing excel files
library(mapview) # for seeing the profiles in a map
library(sf) # to manage spatial data (shp vectors) 

# install.packages("devtools") 
# devtools::install_bitbucket("brendo1001/ithir/pkg") #install ithir package
library(ithir) # for horizon harmonization

```

The next step is to load the national soil data into *R Studio*. For that, it is recommendable to have the data in either Microsoft Excel format (.xlsx) or as comma separated value table (.csv). In both cases, each row represent a sample (or horizon) and each column represent a variable. Then, the datasets can be loaded from the specified folder using the respective functions specified in the code below. It is noteworthy that in **R** datasets also need to be assigned to a user-defined variable in order to be saved in the "global environment".

After reading in the file, the package `tidyverse` comes into play. By using the `select()` and `unique()` functions, the user can select only the necessary columns from the table and ensure that no duplicates are included. At this point it may be necessary to rename certain columns, as shown for the Profile and Horizon ID columns in the code below.
Finally, every time new datasets are loaded into **R**, it is recommendable to check the data. Using the `summary()` function, users can see the class of each variable (= column) and descriptive statistics (for numerical variables). Classes are 'character' (`chr`) for text, integer (`int`) for whole numbers, and numeric (`num`) for numeric variables. 


```{r, eval = T, message=F, warning=F}
# 2 - Import national data =====================================================
# Save your national soil dataset in the data folder /01-Data as a .csv file or 
# as a .xlsx file

## 2.1 - for .xlsx files -------------------------------------------------------
# Import horizon data 
# hor <- read_excel("01-Data/soil_data.xlsx", sheet = 2)
# # Import site-level data
# site <- read_excel("01-Data/soil_data.xlsx", sheet = 1)

## 2.2 - for .csv files --------------------------------------------------------
# Import horizon data 
hor <- read_csv(file = "Digital-Soil-Mapping/01-Data/soil_profile_data.csv")
site <- select(hor, id_prof, x, y, date) %>% unique()
hor <- select(hor, id_prof, id_hor, top:cec)

# change names of key columns
names(site)
names(site)[1] <- "ProfID"
names(hor)
names(hor)[1] <- "ProfID"
names(hor)[2] <- "HorID"
# scan the data
summary(site)
summary(hor)
```

The selection of useful columns is very important since it ensures that users keep a good overview and a clean environment. Using the `select()` function, it is also possible to rename the variables right away (see code below).

```{r select, eval = T}
# 3 - select useful columns ====================================================
## 3.1 - select columns --------------------------------------------------------
hor <- select(hor, ProfID, HorID, top, bottom, ph=ph_h2o, k, soc, clay, bd, cec)

# the variable ph_h2o was renamed as ph
```
